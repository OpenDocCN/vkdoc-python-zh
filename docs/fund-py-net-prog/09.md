# 九、HTTP 客户端

这是关于 HTTP 的三章中的第一章。在这一章中，你将从一个客户端程序的角度学习如何使用该协议，这个客户端程序想要获取和缓存文档，并且可能还向服务器提交查询或数据。在这个过程中，您将了解协议如何运行的规则。第十章将会介绍 HTTP 服务器的设计和部署。这两章都将考虑协议最原始的概念形式，也就是说，简单地作为获取或发布文档的机制。

虽然 HTTP 可以传送多种类型的文档——图像、pdf、音乐和视频——第十一章研究了使 HTTP 和互联网闻名于世的特殊类型的文档:超文本文档的万维网，它们由于 URL 的发明而相互链接，这也在第十一章中有所描述。在那里，您将了解模板库、表单和 Ajax 支持的编程模式，以及试图将所有这些模式整合到一个易于编程的表单中的 web 框架。

HTTP 版本 1.1 ，当今使用最普遍的版本，在 RFCs 7230–7235 中定义，如果这些章节的文本看起来含糊不清或者让你想知道更多，你应该参考它。对于协议设计背后的理论的更多技术介绍，你可以参考罗伊·托马斯·菲尔丁的著名博士论文“架构风格和基于网络的软件架构的设计”的第五章

现在，您的旅程从这里开始，您将学习查询服务器并获得响应文档。

Python 客户端库

HTTP 协议和它提供的大量数据资源是 Python 程序员长期以来的热门话题，这一点在多年来大量声称比标准库中内置的 urllib 做得更好的第三方客户端中得到了反映。

然而，今天，一个单独的第三方解决方案独树一帜，不仅彻底横扫了竞争者的领域，而且还取代了 urllib，成为想要使用 HTTP 的 Python 程序员的首选工具。这个库是 Requests，由 Kenneth Reitz 编写，由 urllib3 的连接池逻辑提供支持，由 Andrey Petrov 维护。

当你在本章中学习 HTTP 时，你将回到 urllib 和 Requests 来看看当面对每一个 HTTP 特性时，它们做得好和不好的地方。它们的基本接口非常相似——它们都提供了一个 callable，该 callable 打开一个 HTTP 连接，发出一个请求，并在返回一个将它们呈现给程序员的 response 对象之前等待响应头。响应体留在传入套接字的队列中，只有在程序员要求时才读取。

在本章的大部分例子中，我将在一个名为`http://httpbin.org`的小型测试网站上测试这两个 HTTP 客户端库，这个网站由 Kenneth Reitz 设计，你可以通过安装`pip`在本地运行它，然后在一个 WSGI 容器(参见第十章)中运行它，比如 Gunicorn。要在`localhost`端口`8000`上运行它，以便您可以在您自己的机器上尝试本章中的示例，而不需要点击`httpbin.org`的公共版本，只需键入以下命令:

```py
$ pip install gunicorn httpbin requests
$ gunicorn httpbin:app
```

然后，您应该能够用 urllib 和请求获取它的一个页面，看看它们的接口乍一看是如何相似的。

```py
>>> import requests
>>> r = requests.get('http://localhost:8000/headers')
>>> print(r.text)
{
  "headers": {
    "Accept": "*/*",
    "Accept-Encoding": "gzip, deflate",
    "Host": "localhost:8000",
    "User-Agent": "python-requests/2.3.0 CPython/3.4.1 Linux/3.13.0-34-generic"
  }
}
>>> from urllib.request import urlopen
>>> import urllib.error
>>> r = urlopen('http://localhost:8000/headers')
>>> print(r.read().decode('ascii'))
{
  "headers": {
    "Accept-Encoding": "identity",
    "Connection": "close",
    "Host": "localhost:8000",
    "User-Agent": "Python-urllib/3.4"
  }
}
```

已经可以看出两个不同之处，它们是本章内容的良好铺垫。Requests 预先声明它支持 gzip 和 deflate 压缩的 HTTP 响应，而 urllib 对此一无所知。此外，虽然 Requests 已经能够确定将这个 HTTP 响应从原始字节转换为文本的正确解码，但 urllib 库只是返回字节并让您自己执行解码。

在强大的 Python HTTP 客户端方面也有其他尝试，其中许多都致力于变得更像浏览器。他们想超越本章中描述的 HTTP 协议，并引入你将在第十一章中学习的概念，将 HTML 的结构、表单的语义以及当你完成一个表单并点击提交时浏览器应该做的规则结合在一起。例如，图书馆机械化曾流行过一段时间。

然而，最终，网站往往过于复杂，无法与除了完整浏览器之外的任何东西进行交互，因为表单如今之所以有效，只是因为 JavaScript 进行了注释或调整。许多现代表单甚至没有真正的提交按钮，而是激活一个脚本来完成工作。事实证明，控制浏览器的技术比 mechanize 更有用，我会在第十一章中介绍其中一些技术。

本章的目的是让你理解 HTTP，看看它有多少特性是可以通过请求和 urllib 访问的，并帮助你理解当你使用内置于标准库中的 urllib 包时你的操作范围。如果您发现自己无法安装第三方库，而是需要执行高级 HTTP 操作，那么您不仅要查阅 urllib 库自己的文档，还要查阅其他两个资源:它的本周 Python 模块条目和在线*深入 Python*书籍中关于 HTTP 的章节。

```py
http://pymotw.com/2/urllib2/index.html#module-urllib2
http://www.diveintopython.net/http_web_services/index.html
```

这些资源都是在 Python 2 时代编写的，因此调用库`urllib2`而不是`urllib.request`，但是你应该会发现它们仍然是 urllib 笨拙而过时的面向对象设计的基本指南。

端口、加密和成帧

端口 80 是纯文本 HTTP 对话的标准端口。端口 443 是客户端的标准端口，这些客户端希望首先协商一个加密的 TLS 对话(见第六章),然后只在加密建立后才开始说 HTTP 这是一个名为安全超文本传输协议(HTTPS)的协议变体。在加密通道内，HTTP 的传输方式与正常情况下通过未加密的套接字传输完全相同。

正如您将在第十一章中了解到的，从用户的角度来看，在 HTTP 和 HTTPS 之间以及在标准或非标准端口之间的选择通常是通过他们构建或给出的 URL 来表达的。

请记住，TLS 的目的不仅是防止流量被窃听，而且是验证客户端所连接的服务器的身份(此外，如果提供了客户端证书，则允许服务器验证客户端身份作为回报)。如果 HTTPS 客户端不检查服务器提供的证书是否与客户端尝试连接的主机名匹配，请不要使用该客户端。本章涉及的所有客户端都会执行这样的检查。

在 HTTP 中，是客户端先说话，传输一个命名文档的*请求* 。一旦整个请求都在网络上，客户机就一直等待，直到从服务器接收到一个完整的*响应*，该响应或者指出一个错误条件，或者提供有关客户机所请求的文档的信息。至少在目前流行的 HTTP/1.1 版本的协议中，不允许客户机通过同一个套接字发送第二个请求，直到响应完成。

HTTP 中有一个重要的对称:请求和响应使用相同的规则来建立格式和帧。下面是一个请求和响应示例，您可以在阅读下面的协议描述时参考:

```py
GET /ip HTTP/1.1
User-Agent: curl/7.35.0
Host: localhost:8000
Accept: */*

HTTP/1.1 200 OK
Server: gunicorn/19.1.1
Date: Sat, 20 Sep 2014 00:18:00 GMT
Connection: close
Content-Type: application/json
Content-Length: 27
Access-Control-Allow-Origin: *
Access-Control-Allow-Credentials: true

{
  "origin": "127.0.0.1"
}
```

请求是以`GET`开始的文本块。响应从版本`HTTP/1.1`开始，一直到标题下面的空白行，包括三行 JSON 文本。请求和响应在标准中都称为 HTTP *消息* ，每个消息由三部分组成。

*   第一行命名请求中的方法和文档，并命名响应中的返回代码和描述。该行以回车和换行结束(CR-LF，ASCII 码 13 和 10)。
*   由名称、冒号和值组成的零个或多个标头。标头名称不区分大小写，因此它们可以按照客户机或服务器的要求大写。每个标题都以 CR-LF 结尾。然后一个空行结束整个头列表——四个字节 CR-LF-CR-LF 形成一对行尾序列，中间没有任何内容。无论上面是否出现标题，该空行都是必需的。
*   紧跟在结束标题的空行之后的可选正文。您将很快了解到，构建实体有几种选择。

第一行和头每个都由它们的终端 CR-LF 序列构成，整个程序集由结尾的空白行构成一个单元，因此服务器或客户端可以通过调用`recv()`直到四字符序列 CR-LF-CR-LF 出现来发现结尾。对于行和标题可能有多长，事先没有提供警告，所以许多服务器对它们的长度设置了常识性的最大值，以避免当麻烦制造者连接并发送无限长的标题时耗尽 RAM。

如果邮件中附加了正文，则有三种不同的正文框架选项。

最常见的成帧是 Content-Length 头的出现，它的值应该是一个十进制整数，以字节为单位给出正文的长度。这很容易实现。客户端可以简单地循环一个重复的`recv()`调用，直到累积的字节最终等于指定的长度。但是，当动态生成数据时，声明内容长度有时是不可行的，并且直到过程完成时才能知道它的长度。

如果报头指定了“chunked”的传输编码，则激活更复杂的方案它不是预先指定长度，而是以一系列更小的片段交付，每个片段都单独以其长度为前缀。每个块至少包含一个十六进制(与十进制的 Content-Length 头相反！)长度字段、两个字符 CR-LF、精确指定长度的数据块以及两个字符 CR-LF。这些块以最后一个块结束，该块声明它的长度为零——至少是数字零，一个 CR-LF，然后是另一个 CR-LF。

在块长度之后但在 CR-LF 之前，发送者可以插入一个分号，然后指定一个适用于该块的“扩展”选项。最后，在最后一个块给出了它的长度 0 和它的 CR-LF 之后，发送者可以附加一些最后的 HTTP 头。如果您自己正在实现 HTTP，可以参考 RFC 7230 了解这些细节。

Content-Length 的另一个替代方案相当突然:服务器可以指定“Connection: close”， 发送尽可能多或尽可能少的正文，然后关闭 TCP 套接字。这带来了一种危险，即客户端无法判断套接字是因为整个主体被成功传递而关闭，还是因为服务器或网络错误而提前关闭，并且它还通过强制客户端为每个请求重新连接而降低了协议的效率。

(标准规定客户端不能尝试“连接:关闭”技巧，因为这样它就不能接收服务器的响应。难道他们没有听说过套接字上的单向`shutdown()`的概念吗，它允许客户机结束它的方向，同时仍然能够从服务器读回数据？).

方法

HTTP 请求的第一个字指定了客户端请求服务器执行的操作。有两种常见的方法，GET 和 POST，以及一些为服务器定义的不太常见的方法，这些服务器希望向可能访问它们的其他计算机程序呈现完整的文档 API(通常是它们自己已经交付给浏览器的 JavaScript)。

GET 和 POST 这两个基本方法提供了 HTTP 的基本“读”和“写”操作。

*GET* 它不能包含正文。该标准坚持认为，在任何情况下，服务器都不能让客户机用这种方法修改数据。任何附加到路径的参数(参见第十一章了解 URL)只能修改被返回的文档，如在`?q=python`或`?results=10`中，不能要求在服务器上进行修改。GET 不能修改数据的限制允许客户端在第一次尝试被中断的情况下安全地重新尝试 GET，允许 GET 响应被缓存(您将在本章后面了解缓存)，并使 web 抓取程序(参见第十一章)可以安全地访问任意多个 URL，而不必担心它们正在创建或删除它们所遍历的站点上的内容。

*POST* 当客户端要向服务器提交新数据时使用。传统的 web 表单如果不简单地将表单域复制到 URL 中，通常会使用 POST 来传递您的请求。面向程序员的 API 也使用 POST 来提交新的文档、注释和数据库行。因为运行同一个帖子两次可能会在服务器上执行两次操作，就像给一个商家第二次支付 100 美元，所以帖子的结果既不能被缓存以满足将来重复的帖子，也不能在响应没有到达时自动重试帖子。

剩下的 HTTP 方法可以分为 GET 和 POST 两类。

像 GET 这样的方法有 OPTIONS 和 HEAD。*选项*方法这使得客户端可以检查内容类型等内容，而不会产生下载主体的成本。

像 POST 这样的操作是 PUT 和 DELETE，因为它们被期望对服务器存储的内容执行可能是不可逆的更改。正如您从它们的名字中所料， *PUT* 意在传递一个新文档，该文档从此将存在于请求指定的路径中，而 *DELETE* 要求服务器销毁该路径以及与之相关的任何内容。有趣的是，这两种方法——在请求“写入”服务器内容的同时——在某种程度上是安全的，而 POST 却不是:它们是*等幂的*，并且可以根据客户端的需要重试任意多次，因为运行其中任何一种方法一次的效果应该与运行多次的效果相同。

最后，该标准指定了一个调试方法 TRACE 和一个方法 CONNECT ，用于将协议切换到 HTTP 之外的东西(正如你将在第十一章中看到的，它用于打开 WebSockets)。然而，它们很少被使用，而且在任何情况下，它们都与作为 HTTP 核心职责的文档传递无关，而这正是你在本章中学到的。有关它们的更多信息，请参考标准。

注意，标准库的`urlopen()`的一个怪癖是它不可见地选择了它的 HTTP 动词:如果调用者指定了数据参数，则选择 POST，否则选择 GET。这是一个不幸的选择，因为 HTTP 动词的正确使用对于安全的客户端和服务器设计至关重要。对于这些本质上不同的方法来说，`get()`和`post()`的请求选择要好得多。

路径和主机

HTTP 的第一个版本允许请求只包含动词和路径。

```py
GET /html/rfc7230
```

在早期，当每台服务器只托管一个网站时，这种方法工作得很好，但是当管理员希望能够部署大型 HTTP 服务器来服务几十个或几百个网站时，这种方法就失效了。仅给定一个路径，服务器如何猜测用户在 URL 中输入了哪个主机名——尤其是对于像`/`这样通常存在于每个网站上的路径？

解决方案是使至少一个报头(主机报头)成为强制性的。协议的现代版本还在最低限度正确的请求中包括协议版本，其内容如下:

```py
GET /html/rfc7230 HTTP/1.1
Host: tools.ietf.org
```

许多 HTTP 服务器会发出一个客户端错误信号，除非客户端至少提供一个主机头来显示 URL 中使用了哪个主机名。如果没有，结果通常是 400 个坏请求。有关错误代码及其含义的更多信息，请参见下一节。

状态代码

响应行以协议版本开始，而不是像请求行那样以协议版本结束，然后它提供一个标准的状态代码，最后是一个非正式的状态文本描述，以呈现给用户或记录在日志文件中。当一切进展顺利时，状态代码为 200，在这种情况下，响应行通常如下所示:

```py
HTTP/1.1 200 OK
```

因为代码后面的文本只是非正式的，所以服务器可以用 Okay 或 Yippee 替换 OK，或者用服务器运行所在国家的国际化文本替换 OK。

该标准——特别是 RFC 7231——为一般和特殊情况指定了二十多个返回代码。如果你需要了解完整的列表，你可以参考标准。一般来说，200 表示成功，300 表示重定向，400 表示客户端请求是不可理解或非法的，500 表示出现了完全是服务器错误的意外情况。

在这一章中，只有少数几个与你有关。

*   *200 OK* :请求成功。如果一个帖子，它有其预期的效果。
*   *301 永久移动* : 路径虽然有效，但不是所讨论资源的规范路径(尽管它可能是在过去的某个时间点)，客户端应该请求响应的 Location 头中指定的 URL。如果客户端想要缓存，所有未来的请求都可以跳过这个旧的 URL，直接进入新的 URL。
*   *303 See Other* : 客户端可以通过对响应的 Location 头中指定的 URL 进行 GET 来了解这个特定的、唯一的请求的结果。但是，以后任何访问此资源的尝试都需要返回到此位置。正如你将在第十一章中看到的，这个状态对于网站的设计至关重要——任何用 POST 成功提交的表单都应该返回 303，这样客户看到的实际页面就可以用一个安全的、幂等的 GET 操作取而代之。
*   *304 未修改* : 文档正文不需要包含在响应中，因为请求头清楚地表明客户机的缓存中已经有了文档的最新版本(参见“缓存和验证”一节)。
*   *307 临时重定向* : 无论客户端发出什么请求，无论是 GET 还是 POST，都应该针对响应的 Location 头中指定的不同 URL 再次尝试。但是将来任何访问该资源的尝试都需要返回到该位置。在其他事情中，这允许在服务器停机或不可用的情况下将表单传送到备用地址。
*   400 错误请求 : 该请求似乎不是有效的 HTTP。
*   *403 禁止的*:请求中没有密码或 cookie(两者都有，见本章后面)或其他识别数据向服务器证明客户机有权限访问它。
*   找不到 404:路径没有命名现有的资源。这可能是最著名的异常代码，因为用户永远不会看到屏幕上显示的 200 代码；他们看到的是一份文件。
*   *405 不允许的方法* : 服务器识别方法和路径，但是这个特定的方法在针对这个特定的路径运行时没有意义。
*   *500 服务器错误* : 又一个熟悉的状态。服务器想要完成请求，但是由于一些内部错误，现在无法完成。
*   *501 未实现* : 服务器不识别你的 HTTP 动词。
*   *502 错误网关* : 服务器是网关或代理(见第十章)，但是它不能联系它后面的服务器，该服务器应该为该路径提供响应。

虽然具有 3 个 *xx* 状态代码的响应不被期望携带主体，但是 4 个 *xx* 和 5 个 *xx* 响应通常都携带主体——通常提供某种人类可读的错误描述。信息较少的例子通常是编写 web 服务器的语言或框架的未修改的错误页面。服务器作者经常手工制作更多信息的页面，以帮助用户或开发人员知道如何从错误中恢复。

当您正在学习一个特定的 Python HTTP 客户端时，有两个关于状态代码的重要问题要问。

第一个问题是一个库是否自动遵循重定向。如果没有，你必须自己检测 3 个 *xx* 状态码并跟踪它们的位置头。虽然内置于标准库中的低级 httplib 模块会让您自己跟踪重定向，但 urllib 模块会按照标准为您跟踪重定向。Requests 库也做同样的事情，它还为您提供了一个历史属性，列出了将您带到最终位置的一系列重定向。

```py
>>> r = urlopen('http://httpbin.org/status/301')
>>> r.status, r.url
(200, 'http://httpbin.org/get')
>>> r = requests.get('http://httpbin.org/status/301')
>>> (r.status, r.url)
(200, 'http://httpbin.org/get')
>>> r.history
[<Response [301]>, <Response [302]>]
```

如果您愿意，Requests 库还允许您使用一个简单的关键字参数来关闭重定向——这是一个可行的策略，但是如果使用 urllib 的话会困难得多。

```py
>>> r = requests.get('http://httpbin.org/status/301',
...                 allow_redirects=False)
>>> r.raise_for_status()
>>> (r.status_code, r.url, r.headers['Location'])
(301, 'http://localhost:8000/status/301', '/redirect/1')
```

如果您的 Python 程序花时间检测 301 错误并试图在将来避免这些 URL，将会减少您查询的服务器上的负载。如果你的程序保持一个持久的状态，那么它可能能够缓存 301 错误以避免重新访问这些路径，或者直接重写 URL。如果用户以交互方式请求 URL，那么您可以打印一条有用的消息，通知他们页面的新位置。

两个最常见的重定向涉及前缀`www`是否属于您用来联系服务器的主机名的前面。

```py
>>> r = requests.get('http://google.com/')
>>> r.url
'http://www.google.com/'
>>> r = requests.get('http://www.twitter.com/')
>>> r.url
'https://twitter.com/'
```

在这个问题上，两个受欢迎的网站对前缀是否应该成为他们官方主机名的一部分采取了相反的立场。然而，在这两种情况下，他们都愿意使用重定向来加强他们的偏好，同时也防止他们的网站出现混乱，出现在两个不同的 URL 上。除非您的应用小心地学习这些重定向并避免重复它们，否则如果您的 URL 是从错误的主机名构建的，您将最终对您获取的每个资源执行两个 HTTP 请求，而不是一个。

关于您的 HTTP 客户端，需要研究的另一个问题是，如果获取 URL 的尝试失败，并显示 4 *xx* 或 5 *xx* 状态代码，它会选择如何提醒您。对于所有这样的代码，标准库`urlopen()`会引发一个异常，使得你的代码不可能意外地处理一个从服务器返回的错误页面，就像它是正常数据一样。

```py
>>> urlopen('http://localhost:8000/status/500')
Traceback (most recent call last):
...
urllib.error.HTTPError: HTTP Error 500: INTERNAL SERVER ERROR
```

如果`urlopen()` 用一个异常打断了你，你如何检查响应的细节呢？答案是通过检查 exception 对象，它执行双重任务，既是一个异常，又是一个带有头和体的响应对象。

```py
>>> try:
...    urlopen('http://localhost:8000/status/500')
... except urllib.error.HTTPError as e:
...    print(e.status, repr(e.headers['Content-Type']))
500 'text/html; charset=utf-8'
```

请求库呈现的情况更令人惊讶——即使是错误状态代码也会导致将响应对象不带注释地返回给调用者。调用者负责测试响应的状态代码，或者自愿调用它的`raise_for_status()`方法 ，这将触发 4 *xx* 或 5 *xx* 状态代码的异常。

```py
>>> r = requests.get('http://localhost:8000/status/500')
>>> r.status_code
500
>>> r.raise_for_status()
Traceback (most recent call last):
...
requests.exceptions.HTTPError: 500 Server Error: INTERNAL SERVER ERROR
```

如果您担心每次调用`requests.get`时都必须记住执行状态检查，那么您可以考虑编写自己的包装函数来自动执行检查。

缓存和验证

HTTP 包括几个设计良好的机制，让客户机避免重复获取它们经常使用的资源，但它们只有在服务器选择向允许它们的资源添加头时才起作用。对于服务器作者来说，考虑缓存并尽可能地允许缓存是很重要的，因为它减少了网络流量和服务器负载，同时也让客户端应用运行得更快。

RFCs 7231 和 7232 详尽地描述了所有这些机制。本节仅试图提供一个基本的介绍。

当服务架构师想要添加头来打开缓存时，他们可以问的最重要的问题是，两个请求是否真的应该仅仅因为它们的路径相同而返回相同的文档。关于一对请求，有没有其他的事情会导致它们需要返回两个不同的资源？如果是这样，那么服务需要在每个响应中包含一个 Vary header ，列出文档内容所依赖的其他头。常见的选择有`Host`、`Accept-Encoding`，如果设计者向不同的用户返回不同的文档，尤其是`Cookie`。

一旦 Vary 头设置正确，就可以激活不同级别的缓存。

可以禁止将资源存储在客户端缓存中，这将禁止客户端在非易失性存储上对响应进行任何类型的自动复制。目的是让用户控制他们是否选择“保存”来将资源的副本存档到磁盘。

```py
HTTP/1.1 200 OK
Cache-control: no-store
...
```

如果服务器选择允许缓存，那么它通常会希望防止这样的可能性，即每次用户请求资源的缓存副本时，客户端可能会一直显示它，直到它变得完全过时。服务器不需要担心资源是否会被永久缓存的一种情况是，它很小心地将给定的路径仅用于文档或图像的一个永久版本。例如，如果每次设计者设计出新版本的公司徽标时，URL 末尾的版本号或散列值都会增加或改变，那么任何给定版本的徽标都可以交付，并允许永久保存。

服务器有两种方法可以防止资源的客户端副本被永久使用。首先，它可以指定一个截止日期和时间,在此之后，如果没有返回给服务器的请求，资源就不能被重用。

```py
HTTP/1.1 200 OK
Expires: Thu, 01 Dec 1994 16:00:00 GMT
...
```

但是使用日期和时间会带来一种危险，即不正确设置的客户机时钟会导致资源的缓存副本被使用太长时间。一个好得多的方法是指定资源一旦被接收就可以被缓存的秒数的现代机制，只要客户机时钟不是简单地停止，这种机制就可以工作。

```py
HTTP/1.1 200 OK
Cache-control: max-age=3600
...
```

此处显示的两个标头授予客户端在有限的时间内继续使用资源的旧副本的单方面能力，而无需与服务器进行任何协商。

但是，如果服务器希望保留对是否使用缓存资源或获取新版本的否决权，该怎么办呢？在这种情况下，它将不得不要求客户端在每次想要使用资源时使用 HTTP 请求进行检查。这将比让客户端静默地使用缓存的副本并且不进行网络操作更昂贵，但是它仍然可以节省时间，因为如果客户端拥有的唯一旧副本确实是过时的，则服务器将不得不发送资源的新副本。

有两种机制,通过这两种机制，服务器可以让客户端检查资源的每次使用，但如果可能的话，让客户端重用其缓存的资源副本。这些在标准中被称为*条件*请求，因为只有当测试显示客户端缓存过期时，它们才会导致主体的传输。

第一种机制要求服务器知道资源最后被修改的时间。这可以很容易地确定资源是否由文件系统上的某个文件支持，但是很难或者不可能确定资源是否是从没有审计日志或者最后修改日期的数据库表中提取的。如果信息可用，服务器可以将它包含在每个响应中。

```py
HTTP/1.1 200 OK
Last-Modified: Tue, 15 Nov 1994 12:45:26 GMT
...
```

想要重用资源的缓存副本的客户机也可以缓存这个日期，然后在下次需要使用资源时将它重复发送给服务器。如果服务器发现自从客户机最后一次接收到资源以来，该资源没有被修改，那么服务器可以通过简单地发送报头和特殊状态码 304 来选择不发送主体。

```py
GET / HTTP/1.1
If-Modified-Since: Tue, 15 Nov 1994 12:45:26 GMT
...
HTTP/1.1 304 Not Modified
...
```

第二种机制处理资源标识，而不是修改时间。在这种情况下，服务器需要某种方法来为资源的每个版本创建一个唯一的标记，该标记保证在每次资源更改时都会更改为一个新的唯一值——校验和或数据库 UUIDs 是此类信息的可能来源。服务器无论何时构建回复，都需要在 e tag 头中传递标签。

```py
HTTP/1.1 200 OK
ETag: "d41d8cd98f00b204e9800998ecf8427e"
...
```

已经缓存并拥有该版本资源的客户端，当它想要再次重用该副本以满足用户动作时，可以向服务器请求该资源，并在它仍然命名该资源的当前版本的情况下包括缓存的标签。

```py
GET / HTTP/1.1
If-None-Match: "d41d8cd98f00b204e9800998ecf8427e"
...
HTTP/1.1 304 Not Modified
...
```

ETag 和 If-None-Match 中使用的引号反映了这样一个事实，即该方案实际上可以进行更强大的比较，而不仅仅是比较两个字符串是否相等。如果您想了解细节，请参考 RFC 7232 第 3.2 节。

再次注意，If-Modified-Since 和 If-None-Match 都仅通过防止再次传输资源来节省带宽，从而也节省了传输所花费的时间。在客户端可以继续使用资源之前，它们仍然至少会产生到服务器的往返行程。

缓存功能强大，对现代 Web 的性能至关重要。然而，默认情况下，您所看到的 Python 客户端库都不会执行缓存。urllib 和 Requests 都认为他们的工作是在需要的时候执行一个真正的实时网络 HTTP 请求，而不是管理一个缓存，这个缓存可能会使您从一开始就不需要通过网络进行对话。如果您想要一个包装器，当指向您可以提供的某种形式的本地持久存储时，它使用 Expires 和 Cache-control 头、修改日期和 ETags 来尝试最小化您的客户机引起的延迟和网络流量，那么您必须寻找第三方库。

如果你正在配置或运行一个代理，缓存也是很重要的，这个话题我将在第十章中讨论。

内容编码

理解 HTTP 传输编码和内容编码之间的区别至关重要。

*传输编码*只是一种将资源转换成 HTTP 响应体的方案。根据定义，传输编码的选择最终没有区别。例如，无论响应是用内容长度编码还是分块编码组织的，客户端都应该发现已经传递了相同的文档或图像。为了加快传输速度，无论字节是原始发送还是压缩发送，资源看起来都应该是一样的。传输编码只是一个用于数据传递的包装，而不是底层数据本身的变化。

尽管现代 web 浏览器支持几种传输编码，但最受程序员欢迎的可能是 gzip。能够接受这种传输编码的客户端必须在 Accept-Encoding 头中声明，并准备好检查响应的传输编码头，以确定服务器是否接受了它的提议。

```py
GET / HTTP/1.1
Accept-Encoding: gzip
...

HTTP/1.1 200 OK
Content-Length: 3913
Transfer-Encoding: gzip
...
```

urllib 库不支持这种机制，所以它需要您自己的代码来生成和检测这些头，然后如果您想利用压缩的传输编码，就自己解压缩响应体。

Requests 库自动声明一个 Accept-Encoding`gzip,deflate`，如果服务器响应一个适当的 Transfer-Encoding，它会自动解压缩主体。这使得压缩在服务器支持时是自动的，并且对请求的用户是不可见的。

内容协商

*内容类型*和*内容编码*，与传输编码相反，对于执行 HTTP 请求的最终用户或客户端程序是完全可见的。它们决定了选择何种文件格式来表示给定的资源，以及如果格式是文本，将使用何种编码来将文本代码点转换成字节。

这些标题允许不能显示新的 PNG 图像的旧浏览器表明它更喜欢 GIF 和 JPG，并且它们允许以用户已经向他们的 web 浏览器表明他们更喜欢的语言来交付资源。以下是由现代 web 浏览器生成的此类标题的示例:

```py
GET / HTTP/1.1
Accept: text/html;q=0.9,text/plain,image/jpg,*/*;q=0.8
Accept-Charset: unicode-1-1;q=0.8
Accept-Language: en-US,en;q=0.8,ru;q=0.6
User-Agent: Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML)
...
```

首先列出的类型和语言具有最强的首选值 1.0，而在标题中后面列出的类型和语言通常被降级为 q=0.9 或 q=0.8，以确保服务器知道它们不是优于最佳选择的首选。

许多简单的 HTTP 服务和站点完全忽略这些头，而是为它们拥有的资源的每个版本使用单独的 URL。例如，如果一个网站同时支持英语和法语，它的首页可能会有两个版本`/en/index.html`和`/fr/index.html`。相同的公司徽标可能位于路径`/logo.png`和`/logo.gif`的两个位置，并且当用户浏览公司的新闻包时，可能会同时提供给用户下载。RESTful web 服务的文档(参见第十章)通常会指定不同的 URL 查询参数，如`?f=json`和`?f=xml`，用于选择返回的表示。

但是这并不是 HTTP 设计的工作方式。

HTTP 的意图是资源应该有一个路径，不管有多少不同的机器格式或人类语言可以用来呈现它，并且服务器使用这些内容协商头来选择资源。

为什么内容协商经常被忽视？

首先，使用内容协商会使用户无法控制他们的用户体验。再想象一个同时提供英语和法语页面的网站。如果它根据 Accept-Language 标题显示一种语言，而用户希望看到另一种语言，那么服务器无法控制这种情况——它必须建议用户打开 web 浏览器的控制面板，更改他们的默认语言。如果用户找不到该设置怎么办？如果他们从公共终端浏览，并且没有权限设置首选项，该怎么办？

许多网站不是将语言选择的控制权交给一个可能写得不好、不连贯或不容易配置的浏览器，而是简单地构建几个冗余的路径集，每个路径集对应一种他们想要支持的人类语言。当用户第一次到达时，他们可能会检查 Accept-Language 标头，以便自动将浏览器定向到最可能合适的语言。但是如果选择不合适，他们希望用户能够从另一个方向返回浏览。

其次，内容协商经常被忽略(或者与基于 URL 的机制一起强制返回正确版本的内容)，因为 HTTP 客户端 API(无论该 API 是由浏览器中的 JavaScript 使用，还是由其他语言在自己的运行时提供)通常很难控制 Accepts 头。将控制元素放入 URL 内部的路径中令人愉快的一点是，任何使用最原始的工具来获取 URL 的人都可以通过调整 URL 来旋转旋钮。

最后，内容协商意味着 HTTP 服务器必须通过在几个轴之间做出选择来生成或选择内容。您可能会认为服务器逻辑总是可以访问 Accepts 头，但事实并非总是如此。如果不考虑内容协商，服务器端的编程通常会更容易。

但是对于想要支持它的复杂服务来说，内容协商可以帮助减少 URL 的可能空间，同时仍然提供一种机制，通过这种机制，智能 HTTP 客户端可以获得已经按照其数据格式或人类读者的需求呈现的内容。如果您打算使用它，请查阅 RFC 7231，了解各种接受头语法的详细信息。

最后一个麻烦是用户代理字符串。

用户代理根本不应该成为内容协商的一部分，而只是作为一个应急的权宜之计，以解决特定浏览器的局限性。换句话说，它是一种针对特定客户端的精心设计的修复机制，同时允许任何其他客户端毫无问题地访问页面。

但是由客户呼叫中心支持的应用的开发者很快发现，他们可以通过禁止除了单一版本的 Internet Explorer 之外的任何浏览器访问他们的网站来消除兼容性问题，并减少预先支持电话的数量。客户端和浏览器之间的军备竞赛导致了你今天所拥有的非常长的用户代理字符串，正如在`http://webaim.org/blog/user-agent-string-history/`中所描述的那样。

您正在探索的两个客户端库 urllib 和 Requests 都允许您将任何 Accept 头放入请求中。它们还都支持自动使用您喜欢的标题创建客户端的模式。Requests 将这个特性构建到了它的`Session`概念中。

```py
>>> s = requests.Session()
>>> s.headers.update({'Accept-Language': 'en-US,en;q=0.8'})
```

所有对类似于`s.get()`的方法的后续调用都将使用这个缺省的头值，除非它们用一个不同的值覆盖它。

urllib 库提供了自己的模式来设置可以注入默认头的默认处理程序，但是，由于它们错综复杂，而且是面向对象的，我建议您参考文档。

内容类型

一旦服务器检查了来自客户端的各种 Accepts 头，并决定传递哪种资源表示，它就相应地设置传出响应的 Content-Type 头。

内容类型是从已经为作为电子邮件消息的一部分传输的多媒体建立的各种 MIME 类型中选择的(参见第十二章)。类型`text/plain`和`text/html`以及图像格式`image/gif`、`image/jpg`和`image/png`都很常见。文件可以按类型发送，包括`application/pdf`。一个简单的字节序列被赋予了`application/octet-stream`的内容类型，对于这个序列，服务器不能保证没有更具体的解释。

在处理通过 HTTP 传递的内容类型头时，有一个复杂的问题需要注意。如果主要类型(斜线左边的单词)是`text`，那么服务器有许多关于如何编码这些文本字符以传输到客户机的选项。它通过在 Content-Type 头后面附加一个分号和一个用于将文本转换成字节的字符编码声明来声明它的选择。

```py
Content-Type: text/html; charset=utf-8
```

这意味着，如果不首先检查分号字符并将其分成两部分，就不能简单地将 Content-Type 头与 MIME 类型列表进行比较。大多数图书馆在这里不会给你任何帮助。无论您使用 urllib 还是使用 Requests，如果您编写需要检查内容类型的代码，您都必须负责在分号上进行拆分(尽管如果您向其`Response`对象请求已经解码的`text`属性，请求至少会使用内容类型的 charset 设置，如果不告诉您的话)。

本书中唯一允许内容类型和字符集分别操作的库是 Ian Bicking 的 WebOb 库 ( 第十章)，它的`Response`对象提供了单独的属性`content_type`和`charset`，这些属性按照标准用分号放在内容类型头中。

HTTP 认证

正如单词 *authentic* 表示真实的、真实的、实际的或真实的东西一样，*认证* 描述了确定请求是否真的来自被授权的人的任何程序。正如您与银行或航空公司的电话交谈会以关于您的地址和个人身份的问题为前缀，以确定这确实是帐户持有人打来的电话一样，HTTP 请求通常也需要携带关于发出请求的机器或人员的身份的内置证明。

未授权的错误代码 401 由服务器使用，这些服务器希望通过协议本身正式发出信号，表明它们无法验证您的身份，或者身份是正确的，但无权查看此特定资源。

许多真实世界的 HTTP 服务器实际上从来不会返回 401，因为它们纯粹是为人类用户设计的。在这些服务器上，试图在没有正确标识的情况下获取资源很可能会将 303 See Other 返回到他们的登录页面。这对人来说很有帮助，但对 Python 程序来说就没那么有帮助了，因为 Python 程序必须学会区分 303 See Other(真正表明身份验证失败)和无害的重定向(实际上只是试图将您带到资源)。

因为每个 HTTP 请求都是独立的，并且独立于所有其他请求，甚至是在同一套接字上紧随其后的请求，所以任何认证信息都必须在每个请求中单独携带。这种独立性使得代理服务器和负载平衡器可以安全地在任意多的服务器之间分发 HTTP 请求，甚至是通过同一个套接字到达的请求。

您可以阅读 RFC 7235 来了解最新的 HTTP 认证机制。早期的最初步骤并不令人鼓舞。

第一种机制，基本认证(或“基本认证”)，包括服务器在其 401 未授权头中包含一个称为*领域* 的字符串。领域字符串允许单个服务器使用不同的密码保护其文档树的不同部分，因为浏览器可以跟上哪个用户密码与哪个领域相匹配。然后，客户机用一个授权头重复它的请求，这个授权头给出了用户名和密码(base-64 编码，好像这样会有帮助)，理想情况下，它会得到一个 200 的回复。

```py
GET / HTTP/1.1
...

HTTP/1.1 401 Unauthorized
WWW-Authenticate: Basic realm="engineering team"
...

GET / HTTP/1.1
Authorization: Basic YnJhbmRvbjphdGlnZG5nbmF0d3dhbA==
...

HTTP/1.1 200 OK
...
```

明文传递用户名和密码在今天听起来是不合理的，但在那个更早、更天真的时代，还没有无线网络，交换设备往往是固态的，而不是运行可能被入侵的软件。随着协议设计者开始考虑这些危险，一个更新的“摘要访问认证”方案被创建，其中服务器发出一个挑战，而客户端用挑战加密码的 MD5 散列来代替。但结果仍然是一场灾难。即使使用了摘要式身份验证，您的用户名仍然清晰可见。所有提交的表单数据和从网站返回的所有资源都清晰可见。然后，一个足够有野心的攻击者可以发起中间人攻击，这样，你就认为他们是服务器，签署了他们自己刚刚从服务器收到的挑战，他们可以利用这个挑战来冒充你。

如果银行想显示你的余额，如果亚马逊想让你输入信用卡信息，网站就需要真正的安全性。因此，SSL 被发明出来创造了 HTTPS，随后是你今天喜欢的各种版本的 TLS，详见第六章。

TLS 的加入意味着，原则上，基本 Auth 不再有任何问题。许多简单的受 HTTPS 保护的 API 和 web 应用现在都在使用它。只有当您构建一系列要安装在 URL 打开器中的对象时，urllib 才支持它(有关详细信息，请参阅文档)，而 Requests 支持带有单个关键字参数的基本 Auth。

```py
>>> r = requests.get('http://example.com/api/',
...                 auth=('brandon', 'atigdngnatwwal'))
```

您还可以准备一个请求`Session`进行认证，以避免自己对每个`get()`或`post()`重复请求。

```py
>>> s = requests.Session()
>>> s.auth = 'brandon', 'atigdngnatwwal'
>>> s.get('http://httpbin.org/basic-auth/brandon/atigdngnatwwal')
<Response [200]>
```

请注意，这种由 Requests 或其他现代库实现的机制并不是成熟的协议！先前指定的用户名和密码没有绑定到任何特定领域。没有 401 响应甚至可以提供一个领域，因为用户名和密码是随请求单方面提供的，没有首先检查服务器是否需要它们。`auth`关键字参数，或者等效的`Session`设置，仅仅是一种设置授权头的方法，而不必自己进行任何 base-64 编码。

现代开发人员更喜欢这种简单性，而不是完全基于领域的协议。通常，他们的唯一目标是对面向程序员的 API 的 GET 或 POST 请求进行独立的身份验证，以确定发出请求的用户或应用的身份。单边授权头非常适合这种情况。它还有另一个优点:当客户已经有充分的理由相信将需要密码时，获得初始 401 不会浪费时间和带宽。

如果您最终与一个真正的遗留系统对话，该系统需要您在同一台服务器上对不同的领域使用不同的密码，那么 Requests 对您没有任何帮助。这将取决于你使用正确的密码和正确的网址。这是一个 urllib 能够做正确的事情而 Requests 不能的罕见领域！但是我从来没有听到过对请求中这一缺点的抱怨，这表明真正的基本授权协商已经变得多么罕见。

饼干

如今，以 HTTP 为媒介的认证已经很少见了。最终，对于 HTTP 资源来说，这是一个失败的提议，因为 HTTP 资源是为使用 web 浏览器的人设计的。

HTTP 认证和用户有什么问题？网站设计者通常希望以自己的方式执行自己的身份验证。他们想要一个自定义的友好的登录页面，遵循他们自己的用户交互准则。当被要求进行协议内 HTTP 认证时，web 浏览器提供的可怜的小弹出窗口是侵入性的。即使在最好的情况下，它们的信息量也不是很大。他们让用户完全脱离了网站的体验。此外，如果没有输入正确的用户名和密码，可能会导致弹出窗口反复出现，而用户不知道发生了什么问题，也不知道如何纠正。

于是饼干被发明了。

从客户端的角度来看， *cookie* 是一个不透明的键值对。它可以在客户端从服务器收到的任何成功响应中传递。

```py
GET /login HTTP/1.1
...

HTTP/1.1 200 OK
Set-Cookie: session-id=d41d8cd98f00b204e9800998ecf8427e; Path=/
...
```

当向该特定服务器发出所有进一步的请求时，客户端会将该名称和值包含在 Cookie 头中。

```py
GET /login HTTP/1.1
Cookie: session-id=d41d8cd98f00b204e9800998ecf8427e
...
```

这使得站点生成登录页面成为可能。当使用无效凭证提交登录表单时，服务器可以根据需要再次显示尽可能多的有用提示或支持链接，所有样式都与站点的其余部分完全一样。一旦表单被正确提交，它就可以授予客户端一个特制的 cookie，以便在所有后续请求中使站点确信用户的身份。

更微妙的是，一个不是真正的 web 表单而是使用 Ajax 停留在同一页面上的登录页面(见第十一章)仍然可以享受 cookies 的好处，如果 API 驻留在相同的主机名上的话。当执行登录的 API 调用确认用户名和密码并返回 200 OK 和一个 cookie 头时，它就授权了对同一站点的所有后续请求——不仅仅是 API 调用，还有对页面、图像和数据的请求——提供 Cookie 并被识别为来自一个经过身份验证的用户。

注意，cookies 应该设计成不透明的。它们应该是随机的 UUID 字符串，引导服务器找到给出真实用户名的数据库记录，或者是加密的字符串，只有服务器可以解密以了解用户身份。如果它们是用户可解析的——例如，如果一个 cookie 有值`THIS-USER-IS-brandon`——那么一个聪明的用户可以编辑 cookie 产生一个伪造的值，并在下一次请求时提交它，以冒充他们知道或能够猜出其用户名的其他用户。

真实世界的 Set-Cookie 头可能比给出的例子复杂得多，如 RFC 6265 中详细描述的那样。我应该提一下`secure`属性。它指示 HTTP 客户端在向站点发出未加密的请求时不要显示 cookie。如果没有这个属性，cookie 可能会被暴露，允许与用户共享咖啡店 wi-fi 的任何其他人了解 cookie 的值，并使用它来冒充用户。有些网站给你一个 cookie 只是为了访问。当你在网站上走动时，他们可以跟踪你的访问。收集到的历史记录已经可以在你浏览时用来定向投放广告，如果你以后用用户名登录，还可以复制到你的永久帐户历史记录中。

如果没有 cookies 跟踪您的身份并证明您已经过身份验证，许多用户导向的 HTTP 服务将无法运行。用 urllib 跟踪 cookies 需要面向对象；请阅读它的文档。如果您创建并持续使用一个`Session`对象，跟踪请求中的 cookies 会自动发生。

连接、保持活动和 httplib

如果连接已经打开，则可以避免启动 TCP 连接的三次握手(见第三章第一部分),这甚至在早期为 HTTP 提供了动力，使连接在浏览器下载 HTTP 资源、JavaScript、CSS 和图像时保持打开。随着 TLS(见第六章)作为所有 HTTP 连接的最佳实践的出现，建立新连接的成本甚至更高，增加了连接重用的好处。

协议版本 HTTP/1.1 将 HTTP 连接设置为在收到请求后保持打开状态。如果客户机或服务器计划在请求完成后挂断，它们可以指定`Connection: close`,否则可以重复使用单个 TCP 连接从服务器获取客户机想要的资源。Web 浏览器通常为每个站点同时创建四个或更多的 TCP 连接，以便可以并行下载一个页面及其所有支持文件和图像，从而尽可能快地将它们呈现在用户面前。

如果您是对细节感兴趣的实现者，应该参考 RFC 7230 的第六部分来了解完整的连接控制方案。

不幸的是，urllib 模块没有提供连接重用。只有使用较低级别的 httplib 模块，才能通过标准库在同一个套接字上发出两个请求。

```py
>>> import http.client
>>> h = http.client.HTTPConnection('localhost:8000')
>>> h.request('GET', '/ip')
>>> r = h.getresponse()
>>> r.status
200
>>> h.request('GET', '/user-agent')
>>> r = h.getresponse()
>>> r.status
200
```

请注意，被挂起的`HTTPConnection`对象不会返回错误，但是当您要求它执行另一个请求时，它会悄悄地创建一个新的 TCP 连接来替换旧的连接。`HTTPSConnection`类提供了同一对象的 TLS 保护版本。

相比之下，Requests library `Session`对象由一个名为 urllib3 的第三方包支持，该包将维护一个到 HTTP 服务器的开放连接的连接池，您最近与这些服务器进行了通信，因此当您从同一站点向它请求另一个资源时，它可以尝试自动重用它们。

摘要

HTTP 协议用于根据资源的主机名和路径获取资源。标准库中的 urllib 客户端可以在简单的情况下工作，但它功能不足，并且缺乏请求的功能，这是 Python 库的一种互联网感觉，是希望从 Web 获取信息的程序员的首选工具。

HTTP 在端口 80 上明文运行，受端口 443 上 TLS 的保护，它在网络上对客户机请求和服务器响应使用相同的基本布局:一行信息，后跟名称-值头，最后是一个空行，然后是可以用几种不同方式编码和定界的正文(可选)。客户端总是先说话，发送请求，然后等待服务器完成响应。

最常见的 HTTP 方法是 GET 和 POST，前者用于获取资源，后者用于向服务器发送更新的信息。还存在其他几种方法，但是它们要么类似 GET，要么类似 POST。服务器为每个响应返回一个状态代码，指示请求是成功了还是失败了，或者客户端是否需要被重定向以加载另一个资源才能完成。

HTTP 内置了几个同心设计层。缓存头可能允许资源被缓存并在客户机上重复使用，而不会被再次提取，或者头可能让服务器跳过重新传送未更改的资源。这两种优化对于繁忙网站的性能都至关重要。

内容协商有希望使数据格式和人类语言符合客户和使用它的人的确切偏好，但它在实践中遇到了问题，使它没有得到普遍应用。内置的 HTTP 身份验证对于交互式使用来说是一个糟糕的设计，已经被定制的登录页面和 cookies 所取代，但是基本的身份验证有时仍然用于对 TLS 安全 API 的请求进行身份验证。

默认情况下，HTTP/1.1 连接可以继续存在并被重用，请求库尽可能小心地这样做。

在下一章中，你将带着你在这里学到的所有知识，反过来看，你将从编写服务器的角度来看待编程的任务。