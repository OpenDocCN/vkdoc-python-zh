# 十三、亚马逊网络服务

亚马逊网络服务，AWS，是一个云平台。它允许使用数据中心的计算和存储资源，按使用付费。AWS 的一个核心原则是，与它的所有交互都应该可以通过 API 实现:可以操纵计算资源的 web 控制台只是 API 的另一个前端。这允许基础设施的自动化配置:所谓的“作为代码的基础设施”，其中计算基础设施是以编程方式保留和操作的。

Amazon Web Services 团队支持 PyPI 上的一个包，`boto3`，用于自动化 AWS 操作。一般来说，这是与 AWS 交互的最佳方式之一。

虽然 AWS 支持控制台 UI，但通常最好将其用作 AWS 服务的只读窗口。当通过控制台 UI 进行更改时，没有可重复的记录。虽然可以记录操作，但这无助于重现它们。

正如我们在前面的章节中所讨论的，将 Jupyter 与`boto3`结合起来，就可以得到一个强大的 AWS 操作控制台。使用`boto3` API 通过 Jupyter 采取的动作可以根据需要重复、自动化和参数化。

当对 AWS 设置进行特别更改以解决问题时，可以将笔记本附加到跟踪问题的票据上，以便清楚地记录为解决问题所做的工作。这既有助于理解在这导致一些不可预见的问题的情况下做了什么，也有助于在再次需要这种解决方案的情况下容易地重复这种干预。

一如既往，笔记本不是一个*审计*解决方案；首先，当允许通过`boto3`访问时，动作不必通过笔记本来执行。AWS 有内部方法生成*审计*日志。笔记本是用来记录意图和允许重复性的。

## 13.1 安全

对于自动化操作，AWS 需要*访问键*。可以为 root 帐户配置访问密钥，但这不是一个好主意。对 root 帐户没有任何限制，所以这意味着这些访问键可以做任何事情。

用于角色和权限的 AWS 平台称为“身份和访问管理”，简称 IAM。IAM 服务负责用户、角色和策略。

一般来说，最好为每个人类用户以及每个需要执行的自动化任务都配备一个单独的 IAM 用户。即使他们都共享一个访问策略，拥有不同的用户意味着更容易进行密钥管理，以及拥有关于谁(或什么)做了什么的准确审计日志。

### 配置访问密钥

通过正确的安全策略，用户可以控制自己的访问密钥。单个“访问密钥”由访问密钥 *ID* 和访问密钥*秘密*组成。 *ID* 不需要保密，它将在生成后通过 IAM 用户界面保持可访问性。例如，这允许通过 ID 禁用或删除访问密钥。

用户最多可以配置两个访问键。拥有两个密钥允许进行 0-停机时间密钥轮换。第一步是生成一个新的密钥。然后到处更换旧钥匙。之后，禁用旧密钥。禁用旧密钥会使任何试图使用它的行为失败。如果检测到这种故障，很容易*重新启用*旧密钥，直到使用该密钥的任务可以升级到新密钥。

经过一段时间后，如果没有观察到故障，删除旧密钥应该是安全的。

一般来说，本地安全策略决定了密钥轮换的频率，但这通常至少应该是每年一次的惯例。一般来说，这应该遵循组织中使用的其他 API 秘密的实践。

注意，在 AWS 中，不同的计算任务可以有自己的 IAM 凭证。

例如，可以为 EC2 机器分配一个 IAM 角色。其他更高级别的计算任务也可以被分配一个角色。例如，运行一个或多个 Docker 容器的弹性容器服务(ECS)任务可以被分配一个 IAM 角色。所谓的“无服务器”Lambda 函数运行在按需分配的基础设施上，也可以被分配 IAM 角色。

如果从这样的任务运行，`boto3`客户机将自动使用这些凭证。这消除了显式管理凭据的需要，并且通常是更安全的替代方法。

### 创建短期代币

AWS 支持所谓的“短期令牌”或 STS。短期令牌可以用于多种用途。它们可用于将替代的身份验证方法转换成可用于任何基于`boto3`的程序的令牌，例如，通过将它们放入环境变量中。

例如，在配置了基于 SAML 的基于 SSO 的身份验证的帐户中，可以调用`boto3.client('sts').assume_role_with_saml`来生成短期安全令牌。这可以在`boto3.Session`中使用，以便获得具有这些权限的会话。

```py
import boto3

response = boto3.client('sts').assume_role_with_saml(
    RoleArn=role_arn,
    PrincipalArn=principle_arn,
    SAMLAssertion=saml_assertion,
    DurationSeconds=120
)
credentials = response['Credentials']
session = boto3.Session(
    aws_access_key_id=credentials['AccessKeyId'],
    aws_secret_access_key=credentials['SecretAccessKey'],
    aws_session_token=credentials['SessionToken'],
)
print(session.client('ec2').describe_instances())

```

一个更现实的用例是在一个定制的 web 门户中，该门户被认证到一个 SSO 门户。它可以代表用户执行操作，而*本身*对 AWS 没有任何特殊的访问权限。

在配置了跨帐户访问的帐户上，`assume_token`可以返回授权帐户的凭证。

即使使用单一帐户，有时创建短期令牌也很有用。例如，这可以用于限制权限；可以创建一个具有有限安全策略的 STS。在一段更容易受到攻击的代码中使用这些限制标记，例如，由于直接的用户交互，允许限制攻击面。

## 13.2 弹性计算云(EC2)

弹性计算云(EC2)是 AWS 中访问计算(CPU 和内存)资源的最基本方式。EC2 运行各种类型的“机器”。其中大多数是“虚拟机”(VM)，与其他 VM 一起运行在物理主机上。AWS 基础设施负责以公平的方式在虚拟机之间分配资源。

EC2 服务还处理机器正常工作所需的资源:操作系统映像、附加存储和网络配置等。

### 区域

EC2 机器在“区域”中运行区域通常有一个友好的名称(如“俄勒冈州”)和一个用于程序的标识符(如“us-west-2”)。

美国有几个地区:在撰写本文时，北弗吉尼亚(“美国东部-1”)、俄亥俄州(“美国东部-2”)、北加利福尼亚(“美国西部-1”)和俄勒冈州(“美国西部-2”)。欧洲、亚太地区等也有几个地区。

当我们连接到 AWS 时，我们连接到我们需要操作的区域:`boto3.client("ec2", region_name="us-west-2")`返回一个连接到俄勒冈州 AWS 数据中心的客户端。

可以在环境变量和配置文件中指定默认区域，但最好是在代码中显式指定(或者从更高级别的应用配置数据中检索)。

EC2 机器也在*可用性区域*中运行。请注意，虽然区域是“客观的”(每个客户都认为区域是相同的)，但可用性区域不是:一个客户的“us-west-2a”可能是另一个客户的“us-west-2c”

亚马逊将所有 EC2 机器放入某个虚拟私有云(VPC)专用网络。对于简单的情况，一个帐户在每个地区有一个 VPC，所有属于该帐户的 EC2 机器都在那个 VPC。

*子网*是 VPC 与可用性区域相交的方式。子网中的所有计算机都属于同一个区域。一个 VPC 可以有一个或多个*安全组*。安全组可以设置关于允许哪些网络连接的各种防火墙规则。

### 亚马逊机器图像

为了启动 EC2 机器，我们需要一个“操作系统映像”虽然可以构建定制的 Amazon 机器映像(AMIs ),但通常情况下我们可以使用现成的映像。

所有主要的 Linux 发行版都有 ami。正确分布的 AMI ID 取决于我们想要运行机器的 AWS *区域*。一旦我们决定了地区和发行版本，我们需要找到 AMI ID。

ID 有时很难找到。如果你有*的产品代码*，比如`aw0evgkw8e5c1q413zgy5pjce`，我们可以用`describe_images`。

```py
client = boto3.client(region_name='us-west-2')
description = client.describe_images(Filters=[{
    'Name': 'product-code',
    'Values': ['aw0evgkw8e5c1q413zgy5pjce']
}])
print(description)

```

CentOS wiki 包含所有相关 CentOS 版本的产品代码。

Debian 镜像的 AMI IDs 可以在 Debian wiki 上找到。Ubuntu 网站有一个工具可以根据地区和版本找到各种 Ubuntu 映像的 AMI IDs。不幸的是，没有集中的自动化注册。可以用 UI 搜索 ami，但这是有风险的；保证阿美族真实性的最好方法是查看创建者的网站。

### SSH 密钥

对于临时管理和故障排除，能够 SSH 到 EC2 机器是很有用的。这可能是手动 SSH，使用 Paramiko、Ansible 或引导 Salt。

构建 ami 的最佳实践是使用`cloud-init`来初始化机器，其默认映像的所有主要发行版都遵循这一实践。`cloud-init`要做的事情之一是允许预先配置的用户通过 SSH 公钥登录，该公钥是从机器的所谓“用户数据”中检索的。

公共 SSH 密钥按地区和帐户存储。添加 SSH 密钥有两种方法:让 AWS 生成一个密钥对，并检索私钥，或者我们自己生成一个密钥对，并将公钥推送给 AWS。

第一种方式通过以下方式完成:

```py
key = boto3.client("ec2").create_key_pair(KeyName="high-security")
fname = os.path.expanduser("~/.ssh/high-security")
with open(fname, "w") as fpout:
    os.chmod(fname, 0o600)
    fpout.write(key["KeyMaterial"])

```

注意，这些键是 ASCII 编码的，所以使用字符串(而不是字节)函数是安全的。

请注意，在输入敏感数据之前，更改文件的权限*是一个好主意。我们还将它存储在一个具有保守访问权限的目录中。*

如果我们想将一个公钥导入 AWS，我们可以这样做:

```py
fname = os.path.expanduser("~/.ssh/id_rsa.pub")
with open(fname, "rb") as fpin:
    pubkey = fpin.read()
encoded = base64.encodebytes(pubkey)
key = boto3.client("ec2").import_key_pair(
    KeyName="high-security",
    PublicKeyMaterial=encoded,
)

```

正如在密码学一章中所解释的，在尽可能少的机器上拥有私钥是最好的。

总的来说，这是一个比较好的办法。如果我们在本地生成密钥并对它们进行加密，那么未加密的私钥泄漏的地方就更少了。

### 启动机器

EC2 客户机上的`run_instances`方法可以启动新的实例。

```py
client = boto3.client("ec2")
client.run_instances(
    ImageId='ami-d2c924b2',
    MinCount=1,
    MaxCount=1,
    InstanceType='t2.micro',
    KeyName=ssh_key_name,
    SecurityGroupIds=['sg-03eb2567']
)

```

API 有点违反直觉——在几乎所有情况下，`MinCount`和`MaxCount`都需要为 1。对于运行几台相同的机器，使用自动缩放组(ASG)要好得多，这超出了本章的范围。总的来说，值得记住的是，作为 AWS 的第一个服务，EC2 拥有最老的 API，在设计良好的云自动化 API 方面学到的经验最少。

虽然通常 API 允许运行多个实例，但这并不常见。`SecurityGroupIds`表示机器在哪个 VPC。当从 AWS 控制台运行机器时，会自动创建一个相当自由的安全组。出于调试目的，使用该安全组是一种有用的快捷方式，尽管通常创建自定义安全组更好。

这里选择的 AMI 是 CentOS AMI。虽然`KeyName`不是强制性的，但是强烈建议创建一个密钥对，或者导入一个密钥对，并使用名称。

`InstanceType`表示分配给实例的计算资源量。`t2.micro`顾名思义，是一台相当微型的机器。它主要用于原型开发，但通常不能支持除了最少量的生产工作负载之外的所有工作负载。

### 安全登录

当通过 SSH 登录时，最好事先知道我们期望的公钥是什么。否则，中介可以劫持连接。尤其是在云环境中，“首次使用时信任”的方法是有问题的；每当我们制造一台新机器时，都会有许多“第一次使用”。由于虚拟机最好被视为一次性的，豆腐原则没有什么帮助。

检索密钥的主要技术是在实例启动时将密钥写入“控制台”。AWS 为我们提供了一种检索控制台输出的方法:

```py
client = boto3.client('ec2')
output = client.get_console_output(InstanceId=sys.argv[1])
result = output['Output']

```

不幸的是，引导时诊断消息的结构并不好，所以解析必须是临时的。

```py
rsa = next(line
           for line in result.splitlines()
           if line.startswith('ssh-rsa'))

```

我们寻找以`ssh-rsa`开始的第一行。现在我们有了公钥，我们可以用它做几件事。如果我们只是想运行一个 SSH 命令行，并且机器不是只允许 VPN 访问的，我们将想把公共 IP 存储在`known_hosts`中。

这避免了第一次使用时信任(Trust-on-First-Use，豆腐渣)的情况:`boto3`使用认证机构安全地连接到 AWS，因此 SSH 密钥的完整性得到了保证。尤其对于云平台来说，豆腐是一个很差的安全模型。既然创造和摧毁机器如此容易，机器的寿命有时以周甚至天来衡量。

```py
resource = boto3.resource('ec2')
instance = resource.Instance(sys.argv[1])
known_hosts = (f'{instance.public_dns_name},'
               f'{instance.public_ip_address} {rsa}')
with open(os.path.expanduser('~/.ssh/known_hosts'), 'a') as fp:
    fp.write(known_hosts)

```

### 建筑形象

建立自己的形象会很有用。这样做的一个原因是为了加速启动。不需要引导一个普通的 Linux 发行版，然后安装所需的包、设置配置等等，只需要做一次，存储 AMI，然后从这个 AMI 启动实例。

这样做的另一个原因是知道升级时间；运行`apt-get update && apt-get upgrade`意味着在升级时获得最新的包。相反，在 AMI 构建中这样做可以知道所有的机器都是从同一个 AMI 运行的。升级可以通过首先用具有新 AMI 的机器替换一些机器，检查状态，然后替换剩余的机器来完成。网飞等人使用的这种技术被称为“不可变图像”虽然还有其他实现不变性的方法，但这是在生产中成功部署的第一种方法。

准备机器的一种方法是使用配置管理系统。Ansible 和 Salt 都有一个“本地”模式，在本地运行命令，而不是通过服务器/客户端连接。

步骤如下:

*   使用正确的基础映像启动 EC2 机器(例如 vanilla CentOS)。

*   检索安全连接的主机密钥。

*   复制 Salt 代码。

*   复制 Salt 配置。

*   通过 SSH，在 EC2 机器上运行 Salt。

*   最后，调用`client("ec2").create_image`将当前磁盘内容保存为 AMI。

```py
$ pex -o salt-call -c salt-call salt-ssh
$ scp -r salt-call salt-files $USER@$IP:/
$ ssh $USER@$IP /salt-call --local --file-root /salt-files
(botovenv)$ python
...
>>> client.create_image(....)

```

这种方法意味着运行在本地机器或 CI 环境中的简单脚本可以从源代码生成 AMI。

## 13.3 简单存储服务(S3)

简单存储服务(S3)是一种对象存储服务。对象是字节流，可以存储和检索。这可以用来存储备份，压缩日志文件，视频文件，以及类似的东西。

S3 通过*键*(一个字符串)将对象存储在*桶*中。可以存储、检索或删除对象。但是，不能就地修改对象。

S3 存储桶名称必须是全球唯一的，而不仅仅是每个帐户。这种唯一性通常是通过添加账户持有人的域名来实现的，例如`large-videos.production.example.com`。

可以将存储桶设置为公开可用，在这种情况下，可以通过访问由存储桶名称和对象名称组成的 URL 来检索对象。这使得 S3 桶，正确配置，是静态网站。

### 管理存储桶

一般来说，创建存储桶是一个相当罕见的操作。新桶对应新代码*流*，而不是代码*运行*。这部分是因为存储桶需要有唯一的名称。然而，有时自动创建存储桶是有用的，也许对于许多并行测试环境来说。

```py
response = client("s3").create_bucket(
    ACL='private',
    Bucket='my.unique.name.example.com',
)

```

还有其他选择，但通常不需要。其中一些与授予 bucket 权限有关。一般来说，管理 bucket 权限的更好方法是管理所有权限的方式:通过将策略附加到角色或 IAM 用户。

为了列出可能的键，我们可以使用:

```py
response = client("s3").list_objects(
    Bucket=bucket,
    MaxKeys=10,
    Marker=marker,
    Prefix=prefix,
)

```

前两个论点很重要；有必要指定存储桶，最好确保响应具有已知的最大大小。

`Prefix`参数非常有用，尤其是当我们使用 S3 桶来模拟“文件系统”时例如，这就是作为网站的 S3 桶通常的样子。将 CloudWatch 日志导出到 S3 时，可以指定一个前缀，准确地模拟一个“文件系统”虽然桶内部仍然是平的，但我们可以使用类似于`Prefix="2018/12/04/"`的东西来只获取 2018 年 12 月 4 日的日志。

当符合条件的对象多于`MaxKeys`时，响应将被截断。在这种情况下，响应中的`IsTruncated`字段将是`True`，并且`NextMarker`字段将被设置。发送另一个`Marker`设置为返回的`NextMarker`的`list_objects`将检索下一个`MaxKeys`对象。这允许通过响应进行分页，即使面对变化的桶也是一致的，在有限的意义上，我们将至少获得所有在分页时没有变化的*对象*。

为了检索单个对象，我们使用`get_object`:

```py
response = boto3.client("s3").get_object(
    Bucket='string',
    Key='string',
)
value = response["Body"].read()

```

`value`将是一个*字节的*对象。

特别是对于小到中等大小的对象，比如几兆字节，这是一种允许简单检索所有数据的方法。

为了将此类物体推入桶中，我们可以使用:

```py
response = boto3.client("s3").put_object(
    Bucket=BUCKET,
    Key=some_key,
    Body=b'some content',
)

```

同样，这适用于身体都适合内存的情况。

正如我们前面提到的，当上传或下载较大的文件(例如，视频或数据库转储)时，我们希望能够增量上传，而不是一次将整个文件保存在内存中。

`boto3`库使用`∗_fileobj`方法公开了此类功能的高级接口。

例如，我们可以使用以下方式传输大型视频文件:

```py
client = boto3.client('s3')
with open("meeting-recording.mp4", "rb") as fpin:
    client.upload_fileobj(
        fpin,
        my_bucket,
        "meeting-recording.mp4"
    )

```

我们还可以使用类似的功能来下载大型视频文件:

```py
client = boto3.client('s3')
with open("meeting-recording.mp4", "wb") as fpout:
    client.upload_fileobj(
        fpin,
        my_bucket,
        "meeting-recording.mp4"
    )

```

最后，通常情况下，我们希望对象直接从 S3 转移到 S3，而不通过我们的自定义代码传输数据——但我们不希望允许未经验证的访问。

例如，一个持续集成作业可能会将其工件上传到 S3。我们希望能够通过 CI web 界面下载它们，但是让数据通过 CI 服务器是令人不快的——这意味着该服务器现在需要处理可能更大的文件，而人们会关心传输速度。

S3 允许我们生成“预签名”的网址。这些 URL 可以作为来自另一个 web 应用的链接给出，或者通过电子邮件或任何其他方法发送，并允许对 S3 资源进行有时间限制的访问。

```py
url = s3.generate_presigned_url(
    ClientMethod='get_object',
    Params={
        'Bucket': my_bucket,
        'Key': 'meeting-recording.avi'
    }
)

```

这个网址现在可以通过电子邮件发送给需要观看录像的人，他们将能够下载视频并观看。在这种情况下，我们不用运行 web 服务器。

一个更有趣的用例是允许预先签名的*上传*。这尤其有趣，因为上传文件有时需要 web 服务器和 web 应用服务器之间微妙的交互，以允许发送大型请求。

相反，直接从客户端上传到 S3 允许我们删除所有的中介。例如，这对于使用一些文档共享应用的用户非常有用。

```py
post = boto3.client("s3").generate_presigned_post(
    Bucket=my_bucket,
    Key='meeting-recording.avi',
)
post_url = post["url"]
post_fields = post["fields"]

```

我们可以从代码中使用这个 URL，比如:

```py
with open("meeting-recording.avi", "rb"):
    requests.post(post_url,
                  post_fields,
                  files=dict(file=file_contents))

```

这使我们能够在本地上传会议记录，即使会议记录设备没有 S3 访问凭据。还可以通过`generate_presigned_post`限制文件的最大大小，以限制上传这些文件的未知设备的潜在危害。

请注意，预签名的 URL 可以多次使用。可以使预先签名的 URL 仅在有限的时间内有效，以减少上传后可能改变对象的任何风险。例如，如果持续时间是一秒，我们可以避免检查上传的对象，直到第二秒完成。

## 13.4 摘要

AWS 是一个流行的基础设施即服务平台，通常以按需付费的方式使用。它适用于基础设施管理任务的自动化，而由 AWS 自己维护的`boto3`是实现这种自动化的一种强有力的方法。