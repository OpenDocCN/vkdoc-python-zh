# 七、对应用日志文件执行复杂的搜索和报告

系统管理职责通常包括安装和支持各种应用。这些可能是由开源社区产生的，也可能是内部开发的。在开发这些应用时，也使用了各种各样的语言；如今常见的语言是 Java、PHP、Python、Ruby 和 Perl(是的，有些人仍在使用)。在这一章中，我将讨论用 Java 开发的应用，因为这似乎是大型企业为其 web 应用选择的最常见的语言。Java 应用通常运行在应用服务器容器中，比如 Tomcat、Jetty、Websphere 或 JBoss。

作为系统管理员，您需要知道应用是否正常运行。每个组织良好和结构化的应用都应该将其状态写入一个或多个日志文件；在 Java 世界中，这通常是通过 log4j 适配器完成的。通过观察日志文件，系统管理员可以检测应用中的任何错误和失败，这些错误和失败通常被记录为异常堆栈跟踪。记录完整的异常堆栈跟踪通常表示不可恢复的错误，即应用无法自行处理的错误。如果您没有很多请求，并且应用只是在做一些事情，那么可以手工捕捉这些异常并分析它们。但是，如果您需要管理数百台服务器，并且产生了数十 GB 的信息，那么您肯定需要一些自动化工具来为您收集和分析数据。在这一章中，我将解释我是如何开发名为 Exctractor 的开源工具的(不，这不是一个错别字——这个名字是由两个词组成的，*异常*和*提取器*)以及它是如何工作的。

定义问题

在继续之前，让我们回顾一下这个应用将试图解决的问题。每个程序都将其运行状态写入日志文件。具体记录什么取决于创建应用的开发人员。没有关于记录什么的强制标准，甚至记录的格式也有些不明确。虽然这不是必需的，但是大多数日志条目都有时间戳，并包括指示消息重要性的严重性级别，以及状态消息的实际文本。这不是强制性的，您可能会发现您正在处理的日志文件有更多的属性，甚至可能更少。例如，我遇到的一些应用甚至懒得记录时间戳。

通常，开发良好的 Java 应用在记录状态消息时或多或少遵循相同的标准。通常，消息是由应用编写的状态报告，指示应用当前正在做什么。在应用运行到未定义状态的情况下，它会生成一个异常，该异常通常会记录完整的执行状态信息:调用堆栈。

我已经创建了一个简单的 web 应用，我将在本章中使用它来说明异常引发的各个方面，并分析不同类型的异常。清单 7-1 是这个应用的源代码。您可以用 javac 工具编译它，并从 Tomcat 应用容器中运行它。请注意，此应用仅用作示例，因为它可能允许任何用户访问您系统上的任何文件；唯一的限制是您的文件系统的访问权限机制。

***清单 7-1*** 。说明应用行为的 Java 程序

```py
import java.io.*;
import java.util.*;
import java.text.*;
import javax.servlet.*;
import javax.servlet.http.*;

public class FileServer extends HttpServlet {

    public void doGet(HttpServletRequest request, HttpServletResponse response)
    throws IOException, ServletException
    {
        response.setContentType("text/html");
        PrintWriter out = response.getWriter();

        String fileName = request.getParameter("fn");
        if (fileName != null) {
            out.println(readFile(fileName));
        } else {
            out.println("No file specified");
        }

    }

    private String readFile(String file) throws IOException {
        StringBuilder stringBuilder = new StringBuilder();
        Scanner scanner = new Scanner(new BufferedReader(new FileReader(file)));

        try {
            while(scanner.hasNextLine()) {
                stringBuilder.append(scanner.nextLine() + "\n");
            }
        } finally {
            scanner.close();
        }
        return stringBuilder.toString();
    }
}
```

清单 7-2 是一个 Java 堆栈跟踪的例子，它是由运行在 Tomcat 应用容器中的 web 应用生成的。

***清单 7-2*** 。一个 Java 堆栈跟踪的例子

```py
Jan 18, 2010 8:08:49 AM org.apache.catalina.core.StandardWrapperValve invoke
SEVERE: Servlet.service() for servlet FileServer threw exception
java.io.FileNotFoundException: /etc/this_does_not_exist_1061 (No such file or directory)
    at java.io.FileInputStream.open(Native Method)
    at java.io.FileInputStream.<init>(FileInputStream.java:137)
    at java.io.FileInputStream.<init>(FileInputStream.java:96)
    at java.io.FileReader.<init>(FileReader.java:58)
    at FileServer.readFile(FileServer.java:30)
    at FileServer.doGet(FileServer.java:21)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:690)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter
    (ApplicationFilterChain.java:269)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter
    (ApplicationFilterChain.java:188)
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:210)
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:172)
    at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:117)
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:108)
    at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:151)
    at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:870)
    at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.
    processConnection(Http11BaseProtocol.java:665)
    at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:528)
    at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt
    (LeaderFollowerWorkerThread.java:81)
    at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:685)
    at java.lang.Thread.run(Thread.java:636)
```

如果您仔细观察这个异常，您可能会注意到应用代码试图打开一个文件，但该文件并不存在。显然，一个编写良好的应用应该比抛出异常更优雅地处理像丢失文件这样的简单情况，但有时在应用逻辑中构建对所有可能场景的检查是不可行的。对于更复杂的应用，这可能根本不可能。

为什么我们使用异常

诸如事件和信号之类的语言结构是正常程序流的一部分。相比之下，异常表示在执行程序时出现了错误，比如用错误的参数调用了一个函数，结果无法计算。例如，假设我们有一个将两个数相除并接受它们作为参数的函数。自然地，被零除是不可能的，并且如果这样的函数接收到被零除的指令，它将不知道该做什么。于是一个看似简单的函数变得相当复杂；它必须检查它是否能够将给出的两个数相除，并返回两个值而不是一个值:一个值表示操作是否成功完成，另一个值保存实际结果。或者，如果操作成功，它可以返回一个数字，否则返回一个空对象。无论是哪种情况，调用这个函数的代码现在都必须能够处理数字和空对象，从而将简单的算术结构变成更复杂的“如果”...else”逻辑流程。

这就是例外出现的地方。无法完成正常操作的函数只会引发一个异常，而不是返回一个指示错误的特殊代码。当出现异常时，程序停止执行，Java 环境继续执行异常处理过程。这种异常可以被应用“捕获”。回到除法的例子，整个计算代码可以包装在 Java 的“try”中...catch”结构。然后，不管代码在哪个点失败，也不管具体的函数(比如除法)，代码都会捕捉任何算术异常，并知道计算不可能完成。

例外总是不好的征兆吗？

简短的回答是“没有”，稍微长一点的回答是“看情况”引发异常的原因是发生了意想不到的事情。假设我们有一个 web 应用从我们的服务器提供文件。所有文件都是从外部页面链接的，通常的假设是，创建列表的人只会列出确实存在的文件。但是，作为人，我们都会犯错误，清单的操作者可能会打错字，所以结果链接会指向一个不存在的文件。现在，如果用户点击链接，应用会尝试完全按照要求去做——检索文件。但是文件不存在，所以负责读取文件的代码会失败并抛出一个异常，指出文件不存在。

应用是否应该检查丢失的文件并做出适当的反应？在这个例子中，大概是的；但是在更复杂的情况下，并不总是能够预测每一种可能的结果并编写代码。即使是像我的文件检索服务这样简单的应用，也不可能总是考虑到所有可能出错的地方。

例如，让我们以 Tomcat 用户的身份运行应用，并假设写入文件系统的所有文件都设置了权限，Tomcat 用户可以读取这些文件。这种情况已经持续了很长时间，应用运行得非常完美。一天，一个新的系统管理员加入了团队，在不知情的情况下，部署了一个具有不同用户权限的文件。突然有一个文件访问错误。文件没有丢失，但是使用 Tomcat 用户权限运行的进程无法读取它。开发人员没有想到这种情况，所以没有代码来处理它。这就是异常处理真正有用的地方；应用可能会遇到不同于正常程序流的情况，并且无法处理它，因此代码会引发一个异常，系统管理员或开发人员可以检查出问题的原因。

为什么我们应该分析异常

现在我们知道日志中的异常并不总是一个不好的迹象，这是否意味着我们应该让它们不被处理？我的一般观点是日志文件应该包含尽可能少的异常。偶然的异常意味着发生了异常的事情，我们应该进行调查；但是，如果在一段时间内有类似的例外，这意味着该事件不再是例外，而是司空见惯的事情。因此，需要改变应用，使处理此类事件成为正常程序流程的一部分，而不是一个例外事件。

回到我的文件阅读器示例，我们看到开发人员最初认为可能有一个他需要检查的错误，这是一个丢失的参数，所以他将检查构建到应用逻辑中:

```py
if (fileName != null) {
    out.println(readFile(fileName));
} else {
    out.println("No file specified");
}
```

这是一个好策略，因为有时可能会发生外部引用没有指定任何文件名的情况，但是应用很乐意处理这种情况。

现在，让我们假设这已经运行了很长时间，没有人报告任何问题。但是有一天，您决定查看应用日志文件，并注意到一些以前从未记录过的异常堆栈跟踪:

```py
Jan 18, 2010 8:08:35 AM org.apache.catalina.core.StandardWrapperValve invoke
SEVERE: Servlet.service() for servlet FileServer threw exception
java.io.FileNotFoundException: /etc/this_does_not_exist_2 (No such file or directory)
    at java.io.FileInputStream.open(Native Method)
...
```

这表示用户试图访问一个不存在的文件。您知道到您的 web 服务的唯一链接来自另一个页面，所以您去修复它。但是如何防止这种情况再次发生呢？您的应用没有问题，但是您可能希望检查并改进向外部页面添加新链接的过程，以便它只指向确实存在的文件。是否构建一个用于处理不存在的文件的案例完全取决于您，因为在应用逻辑中，对于什么时候应该处理什么内容并没有严格的规则。我的观点是，如果异常不太可能发生，最好尽可能保持应用逻辑简单。

现在，过了一段时间，您会遇到另一个异常:

```py
Jan 18, 2010 8:07:59 AM org.apache.catalina.core.StandardWrapperValve invoke
SEVERE: Servlet.service() for servlet FileServer threw exception
java.io.FileNotFoundException: /etc/shadow (Permission denied)
    at java.io.FileInputStream.open(Native Method)
...
```

这一次，它表明文件存在，但权限错误。同样，是时候调查为什么会发生这种情况并解决问题的根本原因了——这并不总是应用的问题，但很可能是应用外部的问题。在这种情况下，新的系统管理员更改了文件权限，这破坏了应用。

从这个简单的真实场景中可以看出，应用日志文件中的异常并不一定意味着生成它们的应用有问题。要找到异常日志直接或间接指出的问题的根本原因，作为系统管理员，您需要尽可能多地了解各种指标。异常堆栈跟踪非常有用，但是您也想知道异常何时开始出现在日志文件中。问题的严重程度如何？你有多少例外？如果您正在接收大量的消息，这可能并不是一种例外情况，需要修改应用以将其作为应用逻辑的一部分来处理。

解析复杂的日志文件

解析日志文件(或任何其他非结构化数据集)是一项相当具有挑战性的任务。与 XML 或 JSON 等结构化数据文件不同，普通日志文本文件不遵循任何严格的规则，可能会在没有任何警告的情况下更改。完全由开发应用的人来决定记录什么以及以什么格式记录。在软件的不同版本之间，日志条目的格式甚至可能会发生变化。作为系统管理员，您可能需要协商某种批准程序，这样，如果您自动解析日志，您就不会在文件格式改变时感到惊讶。最好也让开发人员参与进来，这样他们就可以使用和你一样的工具。如果他们使用相同的工具，他们就不太可能弄坏它们。

为了举例说明，我使用 Tomcat 应用服务器生成的 catalina.out 文件。正如您所看到的，应用本身根本不写任何日志消息，所以您将在那里找到的唯一日志条目来自 JVM 和 Tomcat。显然，如果您使用不同的应用容器，比如 Jetty 或 Jboss，您的日志条目可能会看起来不同。即使您使用的是 Tomcat，您也可以覆盖默认行为和消息格式化的方式，所以请查看您正在处理的日志文件，并相应地调整本章中的示例，以便它们与您的环境相匹配。

我们可以在典型的日志文件中找到什么？

在继续编写分析器代码或为其更改任何配置之前，请查看并确定日志文件中的消息类型，并确定如何明确地识别它们。寻找使它们可区分的共同属性。通常，您会看到由应用本身或应用容器生成的标准消息。

这些消息旨在通知您应用的状态。因为这些消息是由应用生成的，它们很可能指示预期的行为，并且它们通知您的每个状态都是正常应用流的一部分。因为我要调查异常，所以我对那种类型的消息不太感兴趣。清单 7-3 是 Tomcat 日志文件的一个片段，显示了“正常”日志消息的样子。

***清单 7-3*** 。catalina.out 中的标准日志消息

```py
Jan 17, 2010 8:18:24 AM org.apache.catalina.core.AprLifecycleListener lifecycleEvent
INFO: The Apache Tomcat Native library which allows optimal performance in production
 environments was not found on the java.l
ibrary.path: /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0/jre/lib/i386/client:/usr/lib/jvm/
java-1.6.0-openjdk-1.6.0.0/jre/lib/i386:
/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0/jre/../lib/i386:/usr/java/packages/lib/i386:/lib:/
usr/libJan 17, 2010 8:18:24 AM org.apache.coyote.http11.Http11BaseProtocol 
initINFO:Initializing Coyote HTTP/1.1 on http-8081Jan 17, 2010 8:18:24 AM
org.apache.catalina.startup.Catalina load
INFO: Initialization processed in 673 ms
Jan 17, 2010 8:18:24 AM org.apache.catalina.core.StandardService start
INFO: Starting service Catalina
Jan 17, 2010 8:18:24 AM org.apache.catalina.core.StandardEngine start
INFO: Starting Servlet Engine: Apache Tomcat/5.5.23
Jan 17, 2010 8:18:24 AM org.apache.catalina.core.StandardHost start
INFO: XML validation disabled
Jan 17, 2010 8:18:25 AM org.apache.catalina.core.ApplicationContext log
INFO: ContextListener: contextInitialized()
Jan 17, 2010 8:18:25 AM org.apache.catalina.core.ApplicationContext log
INFO: SessionListener: contextInitialized()
Jan 17, 2010 8:18:25 AM org.apache.catalina.core.ApplicationContext log
INFO: ContextListener: contextInitialized()
Jan 17, 2010 8:18:25 AM org.apache.catalina.core.ApplicationContext log
INFO: SessionListener: contextInitialized()
Jan 17, 2010 8:18:25 AM org.apache.catalina.core.ApplicationContext log
INFO: org.apache.webapp.balancer.BalancerFilter: init(): ruleChain:
 [org.apache.webapp.balancer.RuleChain: [org.apache.webapp.
balancer.rules.URLStringMatchRule: Target string: News / Redirect URL:
 http://www.cnn.com], [org.apache.webapp.balancer.rules.
RequestParameterRule: Target param name: paramName / Target param value:
 paramValue / Redirect URL: http://www.yahoo.com], [or
g.apache.webapp.balancer.rules.AcceptEverythingRule: Redirect URL:
 http://jakarta.apache.org]]
```

您可以看到所有日志条目都以时间戳开始。这是我将用来检测日志条目的属性之一。另请注意，日志条目可能会跨越多行。因此，每个长条目都以一行开始，该行以时间戳开始，并在检测到另一个带有时间戳的行时结束。请记下这一点，因为这将成为您的应用的设计决策之一。

异常堆栈跟踪日志的结构

清单 7-4 是一个由 JVM 生成的堆栈跟踪的例子。这个堆栈跟踪来自 Tomcat 应用，该应用由于 web.xml 格式错误而无法加载我的 web 应用。因此，它们是正常操作的例外。

***清单 7-4*** 。异常堆栈跟踪的示例

```py
Jan 17, 2010 10:07:04 AM org.apache.catalina.startup.ContextConfig applicationWebConfig
SEVERE: Parse error in application web.xml file at jndi:/localhost/test/WEB-INF/web.xml
org.xml.sax.SAXParseException: The element type "servlet-class" must be terminated
 by the matching end-tag "</servlet-class>".
    at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
    at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source)
    at org.apache.tomcat.util.digester.Digester.parse(Digester.java:1562)
    at org.apache.catalina.startup.ContextConfig.applicationWebConfig
    (ContextConfig.java:348)
    at org.apache.catalina.startup.ContextConfig.start(ContextConfig.java:1043)
    at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:261)
    at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent
    (LifecycleSupport.java:120)
    at org.apache.catalina.core.StandardContext.start(StandardContext.java:4144)
    at org.apache.catalina.startup.HostConfig.checkResources(HostConfig.java:1105)
    at org.apache.catalina.startup.HostConfig.check(HostConfig.java:1203)
    at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:293)
    at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent
    (LifecycleSupport.java:120)
    at org.apache.catalina.core.ContainerBase.backgroundProcess(ContainerBase.java:1306)
    at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.
    processChildren(ContainerBase.java:1570)
    at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.
    processChildren(ContainerBase.java:1579)
    at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.run
    (ContainerBase.java:1559)
    at java.lang.Thread.run(Thread.java:636)
```

与“普通”日志条目一样，这从显示条目创建时间的时间戳开始。也跨越了几行；事实上，大多数堆栈跟踪都相当长，可能包含一百多行，这取决于应用的结构。堆栈跟踪实际上是一个调用堆栈，它打印出整个函数层次结构，一直到遇到异常情况的函数。

Java 异常堆栈跟踪日志的结构无论如何都不是正式的；我只是为了方便起见才把它分开，因为这将有助于我稍后在解析器代码中组织这些日志条目。你应该可以毫不费力地应用同样的结构。

日志条目的第一行我称之为“logline”这一行包含日志条目创建时的时间戳，以及发生异常的模块名称和函数:

```py
Jan 17, 2010 10:07:04 AM org.apache.catalina.startup.ContextConfig applicationWebConfig
```

下面这条线我称之为“头线”这一行实际上不是实际堆栈跟踪的一部分，但它是由“捕获”异常的应用代码打印出来的:

```py
SEVERE: Parse error in application web.xml file at jndi:/localhost/test/WEB-INF/web.xml
```

最后，第三部分包含异常的“主体”。这包括以下所有行，是日志条目的最后一部分。通常最后一行是 Java 线程运行方法。

```py
org.xml.sax.SAXParseException: The element type "servlet-class" must be terminated by the matching end-tag "</servlet-class>".
    at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
    at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source)
    at org.apache.tomcat.util.digester.Digester.parse(Digester.java:1562)
...
at java.lang.Thread.run(Thread.java:636)
```

我已经定义了异常日志条目的结构，但是我如何知道这是一个异常而不是一个正常的日志条目呢？到目前为止，它们看起来都是一样的:它们都有时间戳，并且都跨越了一行或多行。对于一个人来说，这是一个相当明显的区别，您会立即发现异常，但是在异常堆栈跟踪中是否有任何其他的指纹可以用来识别它是一个真正的异常，而不是冗长的日志条目？

如果您查看并比较不同的异常堆栈跟踪，您会注意到一个共性:每个异常堆栈跟踪都提到异常类名。一些例子包括 org.xml.sax.SAXParseException 和 java.io.FileNotFoundException。同样，类名可以是任何东西，但是将单词 *Exception* 附加到类名是一种公认的做法。所以我要用这个作为我的分类器之一。另一个分类器是单词 *java* 。因为我正在处理 Java 程序，所以在大多数情况下，我会有一个或多个来自原生 Java 库的方法。所以我将假设如果我的异常候选包含这两个单词，它很可能是一个实际的异常。但是我不想限制自己，所以我必须确保我的应用结构允许我更改或插入另一个验证方法。

现在我有东西可以操作了:我知道我的日志条目应该是什么样子。我还知道这个异常是什么样子的，以及它与普通日志条目的不同之处。这应该足以实现日志解析器。

处理多个文件

在开始实际解析之前，我需要先读入数据。这听起来可能微不足道，但是如果你想有效地做到这一点，有一些技巧你可能想知道。

首先，您需要决定从哪里获取数据。虽然这似乎是显而易见的，但是请记住日志文件有不同的形状和大小。我希望这个工具足够灵活，这样它就可以应用于不同的情况。为了使事情变得简单，并在实现阶段消除猜测，我将首先列出我将要做出的一些假设和我将要依赖的一些需求:

*   日志文件可以是纯文本，也可以用 bzip2 压缩。
*   日志文件具有扩展名。对于纯文本文件为 log，对于 bzip2 文件为. log.bz2。
*   我需要能够根据日志文件的名称来处理它们的子集。例如，我需要能够使用文件模式 web 服务器；将处理所有与此匹配的文件，但不处理其他文件。
*   所有文件处理的结果应合并到一个报告中。
*   该工具应该对在指定目录或不同目录列表中找到的所有文件进行操作。还应该包括所有子目录中的日志文件。

处理多个文件

给定刚才陈述的需求，我定义两个变量来表示文件搜索调用的模式:

```py
LOG_PATTERN = ".log"
BZLOG_PATTERN = ".log.bz2"
```

文件名模式存储在全局变量 OPTIONS.file_pattern 中。默认情况下，这被设置为一个空字符串，因此它将匹配所有文件名。这个变量是由命令行解析类控制的，我将在这一章的后面讲到。目前，只需注意它可以通过使用-p 或- pattern 选项设置为任何值。

我需要递归地创建一个目录和所有子目录的列表，这样我就可以在其中搜索日志文件。用户将为我提供一个顶级目录列表，我需要将它展开成一个包含所有子目录和子目录的完整树。

参数列表将由 OptionParser 类存储在 ARGS 变量中。Python 的 OS 库中有一个非常方便的函数，叫做 walk。它递归地在每个目录和所有子目录中构建一个文件列表。

让我们建立一个简单的目录结构，看看 os.walk 函数是如何工作的:

```py
$ mkdir -p top_dir_{1,2}/sub_dir_{1,2}/sub_sub_dir
```

这将产生一个三级目录结构:

```py
$ ls -1R
top_dir_1
top_dir_2

./top_dir_1:
sub_dir_1
sub_dir_2

./top_dir_1/sub_dir_1:
sub_sub_dir

./top_dir_1/sub_dir_1/sub_sub_dir:

./top_dir_1/sub_dir_2:
sub_sub_dir

./top_dir_1/sub_dir_2/sub_sub_dir:

./top_dir_2:
sub_dir_1
sub_dir_2

./top_dir_2/sub_dir_1:
sub_sub_dir

./top_dir_2/sub_dir_1/sub_sub_dir:

./top_dir_2/sub_dir_2:
sub_sub_dir

./top_dir_2/sub_dir_2/sub_sub_dir:
```

现在我们可以使用 os.walk 生成相同的输出，如清单 7-5 所示。

***清单 7-5*** 。用 os.walk 递归检索目录列表

```py
$ python
Python 2.6.1 (r261:67515, Jul  7 2009, 23:51:51)
[GCC 4.2.1 (Apple Inc. build 5646)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> for d in os.walk('.'):
...  print d
...
('.', ['top_dir_1', 'top_dir_2'], [])
('./top_dir_1', ['sub_dir_1', 'sub_dir_2'], [])
('./top_dir_1/sub_dir_1', ['sub_sub_dir'], [])
('./top_dir_1/sub_dir_1/sub_sub_dir', [], [])
('./top_dir_1/sub_dir_2', ['sub_sub_dir'], [])
('./top_dir_1/sub_dir_2/sub_sub_dir', [], [])
('./top_dir_2', ['sub_dir_1', 'sub_dir_2'], [])
('./top_dir_2/sub_dir_1', ['sub_sub_dir'], [])
('./top_dir_2/sub_dir_1/sub_sub_dir', [], [])
('./top_dir_2/sub_dir_2', ['sub_sub_dir'], [])
('./top_dir_2/sub_dir_2/sub_sub_dir', [], [])
>>> os.walk('.')
<generator object walk at 0x1004920a0>
>>>
```

如您所见，对 os.walk 的调用返回一个生成器对象。我将在本章后面更详细地讨论生成器，但是现在，请注意它们是可以迭代的对象，就像你可以迭代任何普通的 Python 列表或 tuple 对象一样。

返回结果是一个三元组，包含以下条目:

*   *目录路径*:当前目录，其内容在接下来的两个变量中公开。
*   *目录名*:目录路径中的目录名列表。此列表不包括“.”和' .. '目录。
*   *文件名*:目录路径下的文件名列表。

默认情况下，os.walk 不会跟随指向目录的符号链接。要跟踪符号链接，可以将 followlinks 参数设置为 True，这将指示 os.walk 跟踪它在扫描目录树时遇到的所有符号链接。

我只对目录列表感兴趣，因为我将使用一个不同的函数来过滤出将要处理和分析的文件。只收集三元组结果的第一个元素，我就可以构建目录列表。因此，为了从作为参数列表提供的顶级目录列表中构建所有目录的递归列表，我将编写以下代码:

```py
DIRS = []
for dir in ARGS:
    for root, dirs, files in os.walk(dir):
        DIRS.append(root)
```

现在 DIRS 列表包含了我需要搜索日志文件的所有目录。我需要遍历这个列表，搜索名称满足三个搜索模式的所有文件:LOG_PATTERN 或 BZLOG_PATTERN 和 OPTIONS.file_pattern。

我将使用一种最简单的方法来获取列表，即遍历目录列表，创建一个简单的内容列表，然后将结果与搜索模式进行匹配，并且只使用满足这两个条件的文件。下面的代码就是这样做的，并打开匹配的文件进行读取:

```py
for DIR in DIRS:

    for file in (DIR + "/" + f for f in os.listdir(DIR) if
                         f.find(LOG_PATTERN) != -1 and f.find(OPTIONS.file_pattern) != -1 ):
        if file.find(BZLOG_PATTERN) != -1:
            fd = bz2.BZ2File(file, 'r')
        else:
            fd = open(file, 'r')
```

仔细看看列表结构，它被称为“列表理解”这是一个强大的机制，可以创建你想要遍历的对象列表。通过 list comprehension，您可以快速、优雅地对现有列表进行验证或转换，并立即获得新列表。例如，下面是快速生成 1 到 10 的所有偶数平方列表的方法:

```py
>>> [x**2 for x in range(10) if x % 2 == 0]
[0, 4, 16, 36, 64]
```

列表理解的基本结构是:

```py
[ <operand> /operation/ for <operand> in <list> /if <check  condition>/ ]
```

其中<operand>是用于生成列表的变量，/operation/是您可能需要对结果列表的每个元素执行的可选操作，<list>是您正在迭代的项目列表，/ <check condition="">/是从结果列表中过滤掉不需要的元素的验证过滤器。</check></list></operand>

考虑到这一点，如果我仔细分析我的文件列表结构，我得到的是:

*   结果数组的每个元素都将被构造为 DIR + "/" + f，其中 DIR 是目录名，f 是从 os.listdir()中收集的。
*   变量 f 按顺序分配给调用 os.listdir()返回的列表中的所有元素。
*   只接受满足条件(f.find(LOG_PATTERN)！= -1 和 f.find(OPTIONS.file_pattern)！= -1)，这要求它们同时匹配 LOG_PATTERN 和 OPTIONS.file_pattern。

另外，请注意，您可以使用列表理解来生成列表对象或生成器。如果创建一个生成器，下一个元素值将仅在被请求时导出，例如，在 for 循环中。根据使用情况，这可能比生成整个列表并保存在内存中要快得多，也更节省内存。

使用内置的 Bzip2 库

您可能已经注意到有两条语句创建了文件描述符对象。一个是平面文本日志文件，另一个是用 bzip2 压缩的文件。区别在于日志文件扩展名，在 bzip2 压缩的情况下是. bz2\.

Python 包含一个 bzip2 处理模块，作为一组标准包的一部分。模块中最有用的类是 BZ2File，它实现了处理压缩文件的完整接口。您可以像使用标准的 Open 函数一样使用它。返回的对象是实现标准文件处理操作的文件描述符对象:读、readline、写、writeline、查找和关闭。

因为唯一的区别是文件描述符对象是如何创建的，即使我使用了不同的函数来获取对象，结果还是被赋给了相同的 fd 变量，这个变量将在后面的代码中使用。

遍历大型数据文件

如果我必须读取和处理大量数据，我不能使用简单的方法将所有内容加载到内存中，然后再进行处理。我肯定会在这里处理大量的数据。根据您的具体情况，这可能会有所不同，但是繁忙的系统可能每小时都会生成千兆字节的日志数据。显然，所有这些数据不能一次装入内存。

解决这个问题的方法是使用发电机。generator 函数允许您生成输出(从文件中读取行),而无需将整个文件加载到内存中。如果您只需要逐行读取文件，您实际上不需要封装 readline()函数，因为您可以简单地编写:

```py
f = open('file.txt', 'r')
for line in f:
    print line
```

但是，如果您需要操作文件数据并使用结果，编写自己的生成器函数来执行所需的计算并生成结果可能是个好主意。例如，您可能希望编写一个生成器，在文件中搜索特定的字符串，并打印该字符串以及该字符串之前和之后的几行内容。这就是发电机派上用场的地方。

什么是发电机，它们是如何使用的？

简而言之，Python 生成器是一个潜在可以返回许多值的函数，并且它还能够在返回之间保持自己的状态。这意味着你可以多次调用这个函数，每次它都会返回一个新的结果。每次你随后调用它，它知道它的最后位置，并从该点继续。

以下示例函数生成斐波那契数列:

```py
def f():
    x, y = 0, 1
    while 1:
        yield y
        x, y = y, y+x
```

第一次调用这个函数时，会初始化 x 和 y，进入无限循环。循环中的第一条语句是返回 y 的值(注意，在生成器中，必须使用 yield 语句)。下次调用这个函数时，它将从停止执行并返回值的地方开始，即 yield 语句。下一条语句是用新值重新分配 x 和 y，其中 x 变成旧的 y，新的 y 是旧的 y 和 x 的和。需要注意的是，调用 generator 函数并不返回该函数要计算的值，而是返回实际的 generator 对象。然后，您可以像通常处理列表一样遍历它，或者调用 next()方法，这将得到下一个值:

```py
>>> g = f()
>>> for i in range(10):
...  g.next()
...
1
1
2
3
5
8
13
21
34
55
>>>
```

如你所见，生成器实际上是函数而不是列表，但是它们可以用作列表。有时，就像 Fibonacci 的例子一样，虚拟链表可以是无限的。当生成器有一组有限的结果时，例如文件中的行或数据库查询中的行，它必须引发 StopIteration 异常，这将通知调用者没有更多可用的结果。

您可以使用生成器遍历文件中的所有行。每当您调用 next()函数时，这将有效地返回下一行，而不实际将整个文件加载到内存中。一旦它被定义为一个生成器，你就可以遍历它。

在我的代码中，我有一个 get_suspect()函数，它实际上是一个生成器，从日志文件中返回可能是异常堆栈跟踪的文本摘录。这个函数接受一个生成器作为它的参数，并遍历它，从而检索所有的行。这是怎么做的。

首先，我创建一个生成器，返回文件中的所有行:

```py
g = (line for line in fd)
```

然后，我使用这个生成器来检索函数中的行:

```py
def get_suspect(g):
    line = g.next()
    next_line = g.next()
    while 1:
        <do something with line and next_line>
        yield result
        try:
            line, next_line = next_line, g.next()
        except:
            raise StopIteration
```

我将对 next()的调用放在“try:...except:"子句，因为当到达文件的最后一行时，生成器将引发一个异常。因此，当文件无法读取时，我只需抛出 StopIteration 异常，它向迭代器发出信号，表明生成器已经用尽了所有值。

检测异常

大多数日志条目只包含一行。所以我检测异常日志条目的方法是这样的:

*   忽略所有单行条目。这些很可能来自应用，并且没有堆栈跟踪，因为不可能将完整的堆栈跟踪放在一行中。
*   具有多行的所有日志条目都被视为包含异常堆栈跟踪。
*   异常堆栈跟踪日志条目必须在日志正文中包含单词 *java* 和*异常*。

进行这种两阶段检测的原因是一个简单的检查，如“它是否有不止一行？”非常便宜，并且可以消除大量的日志条目。

检测潜在候选人

在抽象语言中，该函数的算法如下所示:

*   从文件中读入两行。
*   如果第二行与时间戳模式不匹配，则将其添加到结果字符串中。
*   继续读入和追加行，直到时间戳模式匹配。
*   返回结果。
*   重复此操作，直到文件中不再有数据。

正如你在清单 7-6 中看到的，在这里使用生成器函数是一个明显的选择，因为我需要在函数返回包含潜在异常堆栈跟踪的结果字符串后保留内部函数状态。该函数本身接受另一个生成器函数，用于检索文本行。使用这种方法，可以用任何其他能够生成日志行的生成器来替换文件读取生成器。例如，这可能是一个数据库读取函数，甚至是一个监听和接受 syslog 服务消息的函数。

***清单 7-6*** 。检测潜在异常的生成器函数

```py
def get_suspect(g):
    line = g.next()
    next_line = g.next()
    while 1:
        if not (TS_RE_1.search(next_line) or TS_RE_2.search(next_line)):
            suspect_body = line
            while not (TS_RE_1.search(next_line) or TS_RE_2.search(next_line)):
                suspect_body += next_line
                next_line = g.next()
            yield suspect_body
        else:
            try:
                line, next_line = next_line, g.next()
            except:
                raise StopIteration
```

显然，这可以用一个具有更高级的逻辑和更好的命中率的函数来代替，但它同样有效和轻量级。

这里有一些你可能想尝试的想法:

*   不要使用两个预定义的模式进行时间戳检测，而是尝试用预编译模式定义一个列表，它将匹配大多数流行的格式。然后，当函数运行时，它会计算成功的匹配，并动态地重新排列列表，使最受欢迎的匹配首先出现。
*   如果有大量多行日志条目，这种简单的方法将会失败。尝试在日志正文中生成第一行的散列，并将它们存储在一个单独的数据结构中。真正的异常验证函数将根据猜测是否正确，用真/假值更新这个表。然后，这个函数可以根据这个表检查哈希值，因此它将知道哪些重复的日志条目不是真正的异常，尽管它们看起来像异常。

过滤合法的异常跟踪

到目前为止，所有的代码都是标准函数。这主要是因为代码处理选择文件和做一些初始验证。这些任务都与实际的异常处理代码无关。现在，对于异常解析和分析任务，我将使用适当的方法定义一个类。这样我可以把它作为一个完全独立的模块来分发和使用。

例如，假设我想实现一个基于 web 的应用，我的用户可以在其中提交他们的异常日志并获得一些统计数据，我希望能够重用这些代码。打开文件和处理文件模式的功能已经过时，因为根本没有要处理的文件——所有数据都来自 web 服务器。类似地，您可能想要分析存储在数据库中的数据，在这种情况下，您必须编写一个接口来检索这些数据；但是，您仍然可以重用处理异常堆栈跟踪文本的代码。因此，请始终保持代码在逻辑上的分离。

正如我所提到的，我的异常检测机制(清单 7-7 )有些幼稚——我在堆栈跟踪体中检查单词*异常*和 *java* 。

***清单 7-7*** 。验证异常

```py
def is_exception(self, strace):
        if strace.lower().find('exception') != -1 and \
           strace.lower().find('java')      != -1:
            return True
        else:
            return False
```

这很容易改变；如果你需要比这个简单的测试更复杂的东西，你可以重写这个函数来使用一个更适合你的情况的算法。

清单 7-8 显示了这种检测机制是如何与类的其他部分结合在一起的。

***清单 7-8*** 。异常容器类的基本结构

```py
class ExceptionContainer:
    def __init__(self):
        <initialise the object>

    def insert(self, suspect_body, f_name=""):
        lines = suspect_body.strip().split("\n", 1)
        log_l = lines[0]
        if self.is_exception(lines[1]):
            <update exceptions statistics and couners>
```

对于检测到的每个可疑日志行，将调用 insert 方法。然后，该方法将调用验证函数，该函数检查所提供的文本是否实际上是堆栈跟踪，是否应该被计数。

在数据结构中存储数据

我的应用的主要目标是收集日志文件中发生的异常的统计信息；因此，我需要考虑如何以及在哪里存储这些数据。有两种选择:我可以将这些数据保存在内存中，或者将它们转储到数据库中。当在这两者之间做出选择时，我需要问我是否必须做下面的任何一个:

*   在程序终止后，在相同的结构中维护这些数据？
*   长时间保存大量记录，并从任何其他工具访问它们？

如果两个问题的答案都是肯定的，我可能需要使用一个外部数据库来保存统计数据。但是，我不认为日志文件会有大量不同类型的异常。可能有成千上万的异常，但最有可能的是只有几百种异常。很难想象一个应用会产生所有的异常。此外，存储统计数据不是该应用生命周期的一部分。收集和分析这些数据是由一个外部进程来完成的，因此对于这个应用来说，这些数据只需要在计算阶段是“实时”的。

因此。我将使用 Python 的列表数据结构来保存数据，并在以后将其用于报告，但是当应用完成执行时，这些数据将全部丢失。

异常堆栈跟踪数据的结构

没有必要抓住我遇到的每一个例外；我只需要保存每种特定类型的异常的所有事件的计数器，以及该类型的详细信息。如前所述，异常堆栈跟踪可以分解成这些部分:

1.  日志行(带有时间戳的行)
2.  异常标题(异常堆栈跟踪的第一行)
3.  异常体(堆栈跟踪)

除了这些信息，我还需要以下信息:

*   对每种特定类型出现的次数进行计数的计数器。
*   用于快速参考的描述。
*   可用于组织不同类型异常的组。例如，您可能希望有一个组来统计与丢失文件相关的所有异常；但是因为它们可能是由应用的不同部分生成的，甚至是由不同的库生成的，所以您可能需要使用不同的规则来匹配它们。在这里分组是为所有这些异常维护相同计数器的唯一方便的方法。
*   文件名，以便用户知道在哪个文件中发现了异常。如果您正在分析存储在单个目录中的大量文件，这将非常有用。

因此，每当我插入一个新的异常时，下面的字典将被追加到一个列表中:

```py
{ 'count'    : # counter
  'log_line' : # logline
  'header'   : # header line
  'body'     : # body text with stack trace
  'f_name'   : # file name
  'desc'     : # description
  'group'    : # group
}
```

为未知异常生成异常指纹

假设我还没有提供任何分类规则，应用需要能够识别类似的异常，并相应地对它们进行分组。一种可能是存储一个异常正文文本，并与其他文本进行比较。如果下一个异常与存储的匹配，我就增加计数器；否则，我也会把它存储起来，用于将来的比较。图 7-1 是该过程的流程图。

![9781484202180_Fig07-01.jpg](img/00031.jpeg)

图 7-1 。统计例外情况

这是可行的，但会非常慢，因为字符串比较操作在计算能力方面非常慢且昂贵。所以如果可能的话，尽量避免使用它们，尤其是当你需要比较长的字符串时，比如长的文本片段。

执行快速文本-斑点比较更有效的方法是为每个文本片段生成一些唯一的属性，然后比较这些属性。(我说的“唯一”是指在那段特定的文本中是唯一的。)

这种属性可以是数据流的 MD5 散列函数。您可能已经知道，加密哈希函数(MD5 是一个广泛使用的例子)是一个接受任何数据块并返回预定义大小的位串的过程。该字符串的生成方式是，如果原始数据被修改，它也会改变。根据定义，输出字符串可能比输入字符串小得多，所以显然信息丢失了，无法恢复；但是该算法保证，如果两个字符串的哈希值相同，那么原始字符串也很有可能相同。

Python 有一个内置的 MD5 库，可以用来为任何输入数据生成 MD5 和。因此，我将使用这个函数为我遇到的所有异常生成 MD5 散列，然后比较这些字符串，而不是比较全栈跟踪。清单 7-9 摘自插入方法。以下变量在函数的开头定义:

*   log_l:异常日志行
*   hd_l:异常标题行
*   异常体文本
*   f_name:发现异常的文件名
*   self.exception:字典，其中键是异常体文本的总和，值是保存异常堆栈跟踪详细信息的另一个字典

***清单 7-9*** 。生成 MD5 并将其与存储的值进行比较

```py
01:    m = md5.new()
02:    m.update(log_l.split(" ", 3)[2])
03:    m.update(hd_l)
04:    for ml in bd_l.strip().split("\n"):
05:        if ml:
06:            ml = re.sub("\(.*\)", "", ml)
07:            ml = re.sub("\$Proxy", "", ml)
08:            m.update(ml)
09:        if m.hexdigest() in self.exceptions:
10:            self.exceptions[m.hexdigest()]['count'] += 1
11:        else:
12:            self.exceptions[m.hexdigest()] = { 'count'  : 1,
13:                                               'log_line': log_l,
14:                                               'header' : hd_l,
15:                                               'body'  : bd_l,
16:                                               'f_name': f_name,
17:                                               'desc'  : 'NOT IDENTIFIED',
18:                                       'group' : 'unrecognised_'+m.hexdigest(), }
```

下面是这个函数中实际发生的事情的详细解释:

*   第 1–3 行:初始化 md5 对象，并将它分配给异常日志行的第三个字段和整个异常标题行。我只选择异常日志行的最后一个字段的原因是，前两个字段将包含不断变化的日期和时间字符串，所以我不希望它们改变我将要生成的 MD5 散列。
*   第 4–5 行:遍历异常体的所有行，一次一行。
*   第 6–8 行:去掉括号中的所有文本，并删除所有对自动生成的 Java 代理对象的引用。如果行号不同，但异常堆栈跟踪看起来相同，则很有可能实际上它们是相同的。代理对象被分配了序列号，所以它们永远不会有相同的名字；因此，我也需要删除它们，这样 MD5 散列就不会改变。
*   第 9 行:调用 hexdigest 方法，该方法将为使用 update 函数存储的文本生成 MD5 散列，并将结果与所有存储的密钥进行比较。
*   第 10 行:如果有匹配，增加它的计数器。
*   第 11–18 行:否则，插入一条新记录。

检测已知异常

到目前为止，我的应用可以检测唯一的异常，并对它们进行适当的分类。这很有用，但是有一些问题:

*   与任何启发式算法一样，当前的实现在检测和比较异常的方式上非常幼稚。它做得很好，但即使是在非常简单的情况下，如文件未找到异常，也可能会有困难。如果异常是在 Java 应用的不同部分引发的，它将产生完全不同的输出，并且基本上相同类型的异常将被记录多次。有人可能会认为这是预期的行为，您确实需要知道异常是在哪里出现的，这将是一个有效的注释。在其他情况下，您并不真正关心这些细节，而是希望将所有文件未找到错误消息合并到一个组中。目前这是不可能的。
*   命名约定真的很混乱；所有的异常组都将有不可读的名称，比如无法识别的 _ 6 C2 DC 65d 7 c 0 bfb 0768 ddff 8 caba CCF 68。
*   如果异常细节包含特定于时间或特定于请求的信息，则该算法会将这些异常视为不同的，因为无法知道“文件未找到:文件 1.txt”和“文件未找到:文件 2.txt”实际上是同一个异常。为了验证这种行为，我生成了一千多个异常，在这些异常中，请求的文件名是相同的，并且生成了类似数量的具有唯一文件名的错误消息。针对这个示例日志文件运行应用的结果是生成一个包含一千多个实例的组和一千多个包含一个或两个实例的不同组。事实是所有的异常都是同一类型的。
*   虽然我不是在比较大段的文本，但是计算一个 MD5 散列然后比较 has 字符串还是比较慢的。

根据这些结果，我将修改应用，以便它允许我定义如何检测和分类我的异常。

正如您已经知道的，每个异常都分为三个部分:日志行、头和堆栈跟踪体。我将允许用户为任何这些字段定义一个正则表达式，然后使用该正则表达式来检测异常。如果任何一个定义的正则表达式是匹配的，那么异常将被相应地分类；否则，它会被我之前实现的启发式算法进一步处理。我还将允许用户定义他们喜欢的任何分组名称，因此它将比无法识别的 _ 6 C2 DC 65d 7 c 0 bfb 0768 ddff 8 caba CCF 68 字符串更有意义。

配置文件

有许多方法可以存储应用的配置数据。我更喜欢使用 XML 文档，原因如下:

1.  Python 有用于解析 XML 的内置库，因此访问配置数据很简单。
2.  当配置文件被提供给 XML 解析器时，语法验证会自动发生，所以我不需要担心检查配置文件的语法。
3.  XML 文档有一个定义清晰、明确的结构，允许我在需要时实现层次结构。

使用 XML 还有一个实际的缺点——它并不真正对人友好。然而，通过使用适当的可以突出语法的编辑器，我们可以减轻这种情况。现在，大多数编辑器都支持这个功能。几乎所有 Linux 发行版都提供的 ViM 编辑器也能够突出显示 XML 语法。

清单 7-10 是一个简单的配置文件，用于捕捉大多数文件未找到异常。

***清单 7-10*** 。包含两条规则的配置文件

```py
<?xml version="1.0"?>
<config>
    <exception_types>
        <exception logline=""
                   headline=""
                   body="java\.io\.FileNotFoundException: .+ \(No such file or directory\)"
                   group="File not found exception"
                   desc="File not found exception"
        />
        <exception logline=""
                   headline=""
                   body="java\.io\.FileNotFoundException: .+ \(Permission denied\)"
                   group="Permission denied exception"
                   desc="Permission denied exception"
        />
    </exception_types>
</config>
```

配置文件以一个文档标识字符串开始，它告诉解析器这是一个 XML 1.0 版本的文档。对于基本处理，这些信息不是严格要求的，可以省略，但是为了完整性，最好遵循规范。

XML 配置文件的根元素是<config>标记，它包含了所有其他的配置项。现在我可以选择将异常声明直接放在<config>标签中，因为我没有计划在我的配置文件中放入任何其他东西，这样就可以了。但是，如果我后来添加了任何新类型的配置项—例如，影响报告的内容—它在逻辑上就不合适了。因此，创建一个 branch 标记并将给定类型的所有元素放入其中总是一个好主意。因此，我定义了一个新的域元素，我将其命名为<exception_types>。每个单独异常类型的所有声明都将在这里定义。</exception_types></config></config>

如您所见，实际的异常声明非常简单。我有三个正则表达式占位符，后面是描述和组名字段。

用 Python 解析 XML 文件

有两种解析 XML 文档的方法。一种方法叫做 SAX，或者 XML 的简单 API。但是，在用 SAX 处理 XML 之前，需要为每个感兴趣的标记定义一个回调函数。然后调用 SAX 方法来解析 XML。解析器将一次读取 XML 文件的一行，并为每个识别的元素调用一个注册的方法。

我将在示例中使用的另一种方法叫做文档对象模型(DOM)。与 SAX 不同，DOM 解析器将整个 XML 文档读入内存，解析它，并构建该文档的内部表示。本质上，XML 文档表示一种类似树的结构，节点元素包含子元素或分支元素，等等。所以 DOM 解析器构建了一个类似树的链接数据结构，并为您提供了遍历该树结构的方法。

在 XML 文档中查找信息有三个基本步骤:解析 XML 文档，找到包含您感兴趣的元素的树节点，并读取它们的值或内容。

第一步，解析 XML 文档，非常简单，只需要一行代码(如果算上 include 语句，是两行)。下面的代码读入整个配置文件，并创建一个 XML 解析器对象，以后可以用它来查找信息。

```py
from xml.dom import minidom
config = minidom.parse(CONFIG_FILE)
```

下一步是找到所有的<exception>元素。我知道它们的“父”节点是<exception_types>元素，所以我需要先获得它们的列表。这可以通过 getElementsByTagName 方法来完成，该方法可用于任何 XML 对象。该方法接受一个参数——您要查找的元素的名称。结果是具有您搜索的名称的元素对象的列表。method 执行的搜索是递归的，所以如果我从顶层开始(在我的实例中是 document 对象)，它将返回具有这个特定名称的所有元素。在这种情况下，我不妨搜索一下<exception>标签。有了这个简单的配置文件，这个方法也可以工作，但是单词 *exception* 太普通了，因此可能用在 exception_types 部分之外。另一个需要注意的重要事情是，每个元素对象也是可搜索的，并且有相同的方法可供使用。因此，我可以遍历列表中的< exception_types >元素并进一步深入，在每个元素中搜索< exception >标记:</exception></exception_types></exception>

```py
for et in config.getElementsByTagName('exception_types'):
    for e in et.getElementsByTagName('exception'):
```

![Image](img/00010.jpeg) **注意**下面的文字可能看起来有点混乱，因为在术语上有重叠。XML 元素可以有属性，如下例:<element attribute = " attribute value ">element value</element>。类似地，python 对象或类可以具有如下访问的属性:python_object.attribute。当解析 XML 并为您的文档构建表示 Python 对象时，您可以使用 *Python 类属性*来访问 *XML 文档属性*。

现在，我已经找到了我感兴趣的元素，第三步是提取它们的值。从配置文件示例中可以看出，我选择将数据存储为元素属性。每个元素对象中的属性都可以使用名为“属性”的属性来访问这个属性是一个充当字典的对象。字典的每个元素都有两个值: *name* 包含 XML 元素属性的名称， *value* 保存属性的实际文本值。

这听起来可能令人困惑，但如果你看看清单 7-11 中的例子，就会明白了。

***清单 7-11*** 。访问 XML 文档中的配置数据

```py
for et in config.getElementsByTagName('exception_types'):
    for e in et.getElementsByTagName('exception'):
        print e.attributes['logline'].value
        print e.attributes['headline'].value
        print e.attributes['body'].value
        print e.attributes['group'].value
        print e.attributes['desc'].value
```

从这个例子中可以看出，搜索和访问 XML 文档元素的属性确实是一项简单的任务。

存储和应用过滤器

所有异常检测和分类规则都将存储在一个数组中。每个数组元素都是一个字典，包含预编译的正则表达式、组和描述字段，以及一个 ID 字符串，它只是正则表达式字符串的 MD5 散列。这个 ID 可以在以后引用特定的异常组时使用，只要规则没有改变，它就将保持唯一。

使用预编译的正则表达式可以显著提高搜索速度，因为它们已经被验证并转换为准备执行的字节代码。配置解析和导入是在类初始化过程中完成的，正如你在清单 7-12 中看到的例子。

***清单 7-12*** 。类初始化和配置导入

```py
class ExceptionContainer:
    def __init__(self):
        self.filters = []
        config = minidom.parse(CONFIG_FILE)
        for et in config.getElementsByTagName('exception_types'):
            for e in et.getElementsByTagName('exception'):
                m = md5.new()
                m.update(e.attributes['logline'].value)
                m.update(e.attributes['headline'].value)
                m.update(e.attributes['body'].value)
                self.filters.append({ 'id'   : m.hexdigest(),
                                      'll_re':
                                          re.compile(e.attributes['logline'].value),
                                     'hl_re':
                                         re.compile(e.attributes['headline'].value),
                                     'bl_re':
                                         re.compile(e.attributes['body'].value),
                                      'group': e.attributes['group'].value,
                                      'desc' : e.attributes['desc'].value, })
```

当调用 insert 方法(前面有详细描述)时，它将遍历过滤器列表并尝试搜索匹配的字符串。当找到这样的字符串时，将存储异常详细信息，或者增加组的运行计数器，这取决于日志文件中是否已经遇到了该异常。如果没有找到匹配，将执行启发式分类方法，如清单 7-13 所示。

***清单 7-13*** 。匹配自定义分类规则的代码

```py
def insert(self, suspect_body, f_name=""):
    ...

    if self.is_exception(lines[1]):
        self.count += 1
        ...

        logged = False

        for f in self.filters:
            if f['ll_re'].search(log_l) and
                   f['hl_re'].search(hd_l) and
                   f['bl_re'].search(bd_l):
                logged = True
                if f['id'] in self.exceptions:
                    self.exceptions[f['id']]['count'] += 1
                else:
                    self.exceptions[f['id']] = { 'count'    : 1,
                                                 'log_line' : log_l,
                                                 'header'   : hd_l,
                                                 'body'     : bd_l,
                                                 'f_name'   : f_name,
                                                 'desc'     : f['desc'],
                                                 'group'    : f['group'], }
                break

        if not logged:
            # ... unknown exception, try to automatically categorise
```

预编译搜索优于纯文本搜索的优势

我提到过 MD5 散列计算和字符串比较比预编译正则表达式搜索要慢，但是真的是这样吗？让我做些实验来检验这个理论。

首先，我将针对有 4000 多个不同异常的日志文件运行应用，并测量执行时间。文件中有四种类型的异常:几个由 Tomcat 引擎生成的异常，几百个权限被拒绝的异常，一千多个文件名相同的文件找不到，一千多个文件名不同的文件找不到。结果中的第一个数字表示异常的总数，第二个数字表示已识别组的总数:

```py
$ time ./exctractor.py .
4098, 1070

real    0m1.759s
user    0m1.699s
sys     0m0.047s
```

如您所见，浏览文件并统计所有异常花费了将近两秒钟的时间。现在，让我们尝试使用两个简单的规则来检测两种类型的文件未找到和权限被拒绝异常:

```py
$ time ./exctractor.py .
4098, 6

real    0m0.789s
user    0m0.746s
sys     0m0.037s
```

因此，执行时间得到了显著改善，应用只需一半的时间就能完成工作。如果数据集相对较小，并且一些执行时间花费在加载库和读取配置文件上，那么当应用于较大的日志文件时，实际节省的时间甚至会更多。

此外，请注意，超过 1000 个异常组变成了 6 个。这更易于管理，信息量也更大。

生成报告

我现在有了一个全功能的应用，它读入日志文件，解析它们，搜索异常，并根据自动分组或用户定义的类别对类似的异常进行计数。所有这些都很好，但是除非有人能够阅读和分析这些数据，否则这些数据仍然是毫无用处的。

让我们编写一个简单的报告函数，以便将要使用这个应用的人可以从中受益。

将例外分组

如果您密切关注前面讨论异常分组的部分，您可能已经注意到异常不是基于*组*字段进行分组的。而且，如果异常没有在配置文件中分类，它将根据其 MD5 哈希值进行分组；然而，在这种情况下，组名和异常 ID 将有一对一的映射，因为组名是从哈希值生成的:

```py
if m.hexdigest() in self.exceptions:
    self.exceptions[m.hexdigest()]['count'] += 1
else:
    self.exceptions[m.hexdigest()] = { 'count'    : 1,
                                       'log_line' : log_l,
                                       'header'   : hd_l,
                                       'body'     : bd_l,
                                       'f_name'   : f_name,
                                       'desc'     : 'NOT IDENTIFIED',
                                       'group'    : 'unrecognized_'+m.hexdigest(), }
```

但是，如果使用配置文件中的一个过滤器“捕获”了该异常，则会根据过滤器 MD5 哈希值而不是“组”字符串对其进行分类:

```py
if f['ll_re'].search(log_l) and f['hl_re'].search(hd_l) and f['bl_re'].search(bd_l):
    if f['id'] in self.exceptions:
        self.exceptions[f['id']]['count'] += 1
    else:
        self.exceptions[f['id']] = { 'count'    : 1,
                                     'log_line' : log_l,
                                     'header'   : hd_l,
                                     'body'     : bd_l,
                                     'f_name'   : f_name,
                                     'desc'     : f['desc'],
                                     'group'    : f['group'], }
```

这种方法允许您找出每个过滤器被点击的次数，还可以根据“组”字段对计数器进行分组。

所以首先，我需要检查所有记录的异常列表，并创建不同的类别。categories 字典只存储组名和该组中的异常总数。我还使用选项键–v(表示*详细*)来决定是否打印异常细节。清单 7-14 显示了代码。

***清单 7-14*** 。将异常 id 分组到类别中

```py
def print_status(self):
    categories = {}
    for e in self.exceptions:
        if self.exceptions[e]['group'] in categories:
            categories[self.exceptions[e]['group']] += self.exceptions[e]['count']
        else:
            categories[self.exceptions[e]['group']] = self.exceptions[e]['count']
        if OPTIONS.verbose:
            print '-' * 80
            print "Filter ID                 :", e
            print "Exception description     :", self.exceptions[e]['desc']
            print "Exception group           :", self.exceptions[e]['group']
            print "Exception count           :", self.exceptions[e]['count']
            print "First file                :", self.exceptions[e]['f_name']
            print "First occurrence logline :", self.exceptions[e]['log_line']
            print "Stack trace headline      :", self.exceptions[e]['header']
            print "Stack trace               :"
            print self.exceptions[e]['body']
```

为同一数据集生成不同格式的输出

如果没有为详细报告提供选项，应用将只打印两个数字，这两个数字表示发现的异常总数和不同组的总数。您可以使用这些信息快速检查当前状态，也可以累积一段时间的记录，并将数据导入 Excel 或其他工具以绘制漂亮的图表。

如果您计划将报告数据导入到其他应用，它需要符合该应用接受的格式。如果您使用 Excel 创建图表，最方便的导入文件类型是逗号分隔值(CSV ),但是如果您只想在屏幕上显示此信息，您很可能希望它比逗号分隔的一对数字更能提供信息。

因此，我引入了一个选项，允许用户设置他们想要的结果格式:CSV 或纯文本。然后，我创建了两个引用相同变量但提供不同格式的模板字符串:

```py
TPL_SUMMARY['csv']  = "%(total)s, %(groups)s"
TPL_SUMMARY['text'] = "="*80 + "\nTotal exceptions: %(total)s\nDifferent groups: %(groups)s"
```

然后，根据用户提供的格式键，print 语句将选择适当的格式字符串并将变量传递给它:

```py
print TPL_SUMMARY[OPTIONS.format.lower()] % {'total': self.count, 'groups': len(categories)}
```

请注意如何将变量传递给格式化的字符串，并通过名称引用它们。当您需要使用同一组变量产生不同格式的输出时，这种技术非常有用。

计算组统计信息

最后，我想生成一个更详细的报告，显示找到了多少个不同的组，以及每个组中的异常数量，包括相对的(百分比)和绝对的(出现的总数)。

我已经有了字典中的所有细节，包括组名和每个组中的异常总数。但是字典是没有排序的，如果有一个按降序排列的列表就更好了，最糟糕的“冒犯者”在最上面。

Python 有一个非常有用的内置函数，可以对任何可迭代对象进行排序:sorted()。这个函数接受任何可迭代的对象，比如一个列表或字典，并返回一个新的排序列表。棘手的部分是，当遍历一个字典时，你只是遍历它的键，所以当调用 sorted()并把字典作为它的参数时，你只能得到一个排序的键的列表！

```py
>>> d = {'a': 10, 'b': 5, 'c': 20, 'd': 15}
>>> for i in d:
...  print i
...
a
c
b
d
>>> sorted(d)
['a', 'b', 'c', 'd']
>>>
```

显然这不是你真正想要的；您的结果中需要这两个值。字典有一个内置的方法，它将键/值对作为可迭代对象返回—iteritems()。如果您使用这个，您会得到稍微好一点的结果，显示每一对的键和值，但是它们仍然是根据键值排序的，这也不是您想要的:

```py
>>> for i in d.iteritems():
...  print i
...
('a', 10)
('c', 20)
('b', 5)
('d', 15)
>>> sorted(d.iteritems())
[('a', 10), ('b', 5), ('c', 20), ('d', 15)]
>>>
```

sorted()函数接受一个参数，该参数允许您指定一个函数，用于在列表元素是复合元素(如值对)时从列表元素中提取比较键。换句话说，这个函数应该从每一对中返回第二个值。您需要操作符库中的一个特殊函数:itemgetter()。我将使用该函数从每对中提取第二个值，sorted()函数将使用该值对列表进行排序:

```py
>>> from operator import itemgetter
>>> t = ('a', 20)
>>> itemgetter(1)(t)
20
>>> sorted(d.iteritems(), key=itemgetter(1))
[('b', 5), ('a', 10), ('d', 15), ('c', 20)]
>>>
```

最后一步是告诉 sorted()对列表进行逆序排序，这样列表就从值最大的项开始:

```py
>>> sorted(d.iteritems(), key=itemgetter(1), reverse=True)
[('c', 20), ('d', 15), ('a', 10), ('b', 5)]
>>>
```

类似地，我生成并打印例外组的列表。我添加一个统计计算，只是为了显示每个组的相对大小:

```py
for i in sorted(categories.iteritems(), key=operator.itemgetter(1), reverse=True):
    print "%8s (%6.2f%%) : %s" % (i[1], 100 * float(i[1]) / float(self.count), i[0] )
```

摘要

在这一章中，我详细解释了开源工具 Exctractor 是如何编写的，以及每个功能部分是做什么的。本章展示了如何应用 Python 知识构建一个相对复杂的命令行工具来分析大型文本文件。尽管 Python 本身不是一种文本处理语言，但它可以成功地用于这个目的。需要记住的要点:

*   从定义一个问题和你希望你的应用实现什么开始。
*   分析您将使用的数据结构，并根据这些信息做出设计决策。
*   如果您正在处理大型数据集，请尝试通过使用生成器(动态生成值的 Python 函数)来最小化所需的内存量。
*   如果您需要读取和搜索大型数据文件中的信息，请使用生成器构造一次读取一行。
*   Python 内置了对读取和写入压缩文件(如 bzip2 档案)的支持。
*   保持配置的结构化格式，比如 XML，特别是当它包含很多条目的时候。