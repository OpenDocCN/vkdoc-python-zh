# 十二、生物信息学的函数式编程

Python 是一种多范式语言，允许你以多种不同的方式表达计算。它有时被称为**面向对象的** ( **OO** )语言:当然，你可以用 OO 方言写代码，但是你也可以使用其他风格。Python 中的大多数代码都是以命令式风格编写的:没有沿着类层次结构的结构，这是面向对象范式的典型特征，大多数代码都改变了状态，例如，如果你写了`x = x + 1`，你就改变了变量`x`的状态。

如果你写复杂的代码，特别是需要并行处理的代码，命令式和面向对象的范例会遇到一些限制。对于在单台机器上运行的简单脚本，命令式和面向对象式就可以了，但是生物信息学是一个大数据企业，您通常需要纵向扩展和横向扩展，因为有大量信息要处理，并且许多算法计算量很大。

函数式编程对于生物信息学中越来越常见的复杂和并行问题非常有用。许多用于高吞吐量数据分析的现代架构都是基于函数式编程思想的，例如，MapReduce 范式最初是在 Google 和 Apache Spark 等平台上开发的。这些思想也直接适用于 Dask，甚至使顺序代码更加健壮。

函数式编程是基于函数，纯函数求值，避免可变性。在这一章中，我们将采用一种非常实用的方法来介绍这些概念，并举例说明它们在典型的生物信息学应用中的用例。最后，您应该对函数式编程有一个基本的概念理解，但最重要的是，您将理解它的实用性和适用性。

如果您正在使用 Docker，并且因为所有这些库都是数据分析的基础，所以它们都可以在 Docker 映像`tiagoantao/bioinformatics_base`中找到。

在本章中，我们将介绍以下配方:

*   理解纯函数
*   理解不变性
*   避免可变性作为健壮的开发模式
*   使用惰性编程实现流水线操作
*   Python 递归的局限性
*   Python 的`functools`模块展示

# 理解纯函数

一个纯函数有几个重要的属性:对于相同的输入，它产生相同的输出，并且它没有副作用(它不改变全局变量，也不做 I/O)。在这个食谱中，我们将介绍几个简单的例子来阐明这个概念。我们最感兴趣的是第二个特性:没有副作用。在后面的食谱中，将会清楚为什么纯函数会非常有用。

我们将开发一个非常简单的例子，在这个例子中，我们对每个样本中测序的基因进行计数。我们将有一个文本文件的基因计数数据库。例如，我们可能在一个样本上测定了 LCT 和 TP53 的序列，在另一个样本上测定了 LCT、MRAP2 和 POMC 的序列。总数将是:TP53: 1，LCT: 2，MRPA2: 1，POMC: 1。我们将使用一个 CSV 文件，可以很容易地阅读熊猫，甚至只是 CSV 模块。

## 准备就绪

我们将使用一个简单的 CSV 文件作为我们的小数据库。该配方的代码可在`Chapter12/Pure.py`中找到。数据库文件是`my_genes.csv`，在`my_genes.csv.base`有一个保存的原始状态，以防你需要回到原始数据库状态。

## 怎么做...

看看下面的步骤就可以开始了:

1.  让我们首先创建两个简单的函数来加载和保存数据。我们还需要一些时间来恢复原始数据库:

    ```py
    import shutil
    import pandas as pd

    def restore_db(file_name):
        shutil.copyfile(f'{file_name}.base', file_name)

    def load(file_name):
        df = pd.read_csv(file_name).set_index('gene')
        return dict(df['count'])

     def save(dict_db, file_name):
        pd.Series(dict_db).to_csv(
            file_name, index_label='gene', header=['count'])
    ```

我们用熊猫来照顾坚持。这是一个非常简单的例子；您也可以只使用`csv`模块。

1.  我们将考虑三个替代函数来报告在样本中看到的基因；下面是第一个:

    ```py
    def add_sample_csv(gene_list):
        gene_count = load('my_genes.csv')
        for gene in gene_list:
            gene_count[gene]=gene_count(gene,0)+1
        save(gene_count, 'my_genes.csv')
    ```

该函数自动用新样本保存数据。它有读写文件的副作用，所以它不是一个纯粹的函数。

1.  这里有一个第二选择，也报告样本中的基因:

    ```py
    def add_sample_global_dict(gene_list):
        global gene_count
        for gene in gene_list:
            gene_count[gene] = gene_count.get(0) + 1
    ```

这个函数使用模块中的一个全局变量并更新它，这是一个副作用。我们将不使用这个函数，但它是作为一个有副作用的函数的*坏*例子提供的。我们应该避免全局变量。

1.  下面是一个纯函数变体:

    ```py
    def add_sample_dict(dict_db, gene_list):
        for gene in gene_list:
            dict_db[gene] = dict_db.get(0) + 1
        return dict_db
    ```

这个函数改变了它的一个参数——`dict_db`——正如我们将在下一个菜谱中看到的，从函数式编程的角度来看，这不是一个最佳实践。然而，对于相同的输出，它总是返回相同的结果，并且没有副作用，所以这将是一个很好的第一种方法。

1.  假设我们现在运行以下代码:

    ```py
    add_sample_csv(['MC4R', 'TYR'])
    ```

但是，如果我们错误地运行了 10 次，会有什么后果呢？从逻辑角度来看，我们会将两个基因都多报 9 次，从而破坏我们数据库的内容。

1.  作为一种选择，考虑以下:

    ```py
    add_sample_dict(gene_count, ['MC4R', 'TYR'])
    ```

如果运行 10 次，会有什么后果？这仍然是一个错误(因为我们正在改变`gene_count`)，但至少它还没有被提交到磁盘。在进行探索性的数据分析时，这种方言会舒服得多——您可以在知道不会损坏外部数据的情况下运行它。在下一个食谱中，我们将看到一种替代方法，使重新运行代码问题更少。

## 还有更多...

在接下来的几个食谱中，我们将通过非常实际的案例来了解为什么纯函数会非常有用。但是有一个具体的例子很难解释，因此，我们将在这里介绍这个理论。

纯函数使得并行化代码更加容易。想象一个执行分布式计算的框架，比如 Dask 或 Spark。这种框架必须处理硬件故障的潜在情况，代码需要被移动并可能在其他地方重复。如果您的代码在每次运行时都写入磁盘(这是一个副作用的例子)，那么分布式框架的恢复就要复杂得多。如果您的代码没有副作用，那么分布式框架可以重复您的功能，而无需考虑数据一致性。事实上，许多分布式框架不允许在可并行化代码中产生副作用。他们也可能反对数据结构的可变性，这是我们现在要讨论的。

# 理解不变性

函数式编程的另一个共同特点是数据结构通常是不可变的。当你习惯于命令式编程时，这是一个很难理解的概念——命令式编程是指在没有对象的情况下编程，对象的状态会随着时间的推移而改变。在这里，我们将看到一个简单的例子，用一种不可变的方式使前面的配方中的函数工作:也就是说，没有对象被改变，如果我们需要传递新的信息，我们就创建新的。

这个菜谱将从数据结构的角度对不变性做一个简短的介绍。从某种意义上来说，这将是你能在大多数书中找到的标准演示。然而，我们主要考虑的是将可变性作为一种代码设计模式来讨论，这也是下面食谱的主题。但对于这一点，我们需要先理解不变性。

我们将研究两个函数:一个会改变数据结构，另一个不会。这将在我们在本章前面的食谱中遵循的例子的上下文中完成。

## 准备就绪

我们将使用与之前配方相同的数据。该配方的代码可在`Chapter12/Mutability.py`中找到。

## 怎么做...

请遵循以下步骤:

1.  根据前面的配方，我们有一个改变输入字典的函数:

    ```py
    def add_sample_dict(dict_db, gene_list):
        for gene in gene_list:
            dict_db.get(gene,0) +1
    ```

如果你用`add_sample_dict(gene_count, ['DEPP'])`调用这个代码，而没有输出参数，你的字典的`gene_count`将会是*突变的*，以将`1`添加到基因`DEPP`中。如果您运行这个函数的次数过多——这在进行探索性数据分析时很常见——您可能会错误地更改字典。

1.  将前面的实现与下面的进行对比:

    ```py
    def add_sample_new_dict(dict_db, gene_list):
        my_dict_db = dict(dict_db)
        for gene in gene_list:
            my_dict_db[gene] = my_dict_db.get(0) + 1
        return my_dict_db
    ```

在这种情况下，我们将`dict_db`复制到一个新的字典`my_dict_db`，并添加额外的基因列表。制作现有词典的副本需要耗费内存和时间，但是原始词典`dict_db`没有改变。

1.  如果您使用这个实现，您肯定您的输入参数永远不会改变:

    ```py
    new_gene_count = add_sample_new_dict(gene_count, ['DEPP'])
    ```

`gene_count`不是变的，`new_gene_count`是一本全新的词典。您可以重复执行任意多次，而不必担心每次执行的影响。

## 还有更多...

这个简单的方法提供了一个不改变参数的函数的例子。我们现在的情况是，我们可以想执行多少次这个函数就执行多少次——无论是错误的还是故意的——而不会对代码的其余部分产生任何影响。这在我们测试代码或运行探索性数据分析时非常有用，因为我们不必担心副作用。这也方便了分布式执行器(如 Dask 或 Spark)的工作，因为它们不必担心检查现有对象的状态:它们是固定的。

对于一般的软件设计来说，这里也有重要的教训，即使你不使用分布式执行器。这就是我们在下面的食谱中要研究的内容。

# 避免可变性是一种健壮的开发模式

前面的配方引入了不可变数据结构的概念。在这份食谱中，我们将讨论一种设计模式，它可以避免代码中的持久数据库可变性，直到最后。就伪代码而言，长脚本中的大多数应用程序如下工作:

```py
Do computation 1
Write computation 1 to disk or database
Do computation 2
Write computation 2 to disk or database
….
Do computation n
Write computation n to disk or database
```

在这里，我们将展示一个替代范例，并讨论为什么从弹性的角度来看它通常更好:

```py
Do computation 1
Write computation 1 to temporary storage
Do computation 2
Write computation 2 to temporary storage
...
Do computation n
Write computation n to temporary storage
Take all temporary data and write it to definitive disk and database
```

首先，我们将展示这两种方法的代码，然后讨论为什么对于复杂的脚本，后者在大多数情况下是更好的方法。

我们将使用与前面食谱中相同的例子:我们将报告在不同样品中看到的基因。

## 准备就绪

我们将使用与之前配方中相同的数据。该配方的代码可在`Chapter12/Persistence1.py`和`Chapter12/Persistence2.py`中找到。

## 怎么做...

让我们用一些共享代码来设置这两个解决方案。

1.  两种解决方案仍然使用`load`和`save`函数:

    ```py
    import pandas as pd

    def restore_db(file_name):
        shutil.copyfile(f'{file_name}.base', file_name)

    def load(file_name):
        df = pd.read_csv(file_name).set_index('gene')
        return dict(df['count'])

    def save(dict_db, file_name):
        pd.Series(dict_db).to_csv(
            file_name, index_label='gene', header=['count'])
    ```

2.  第一种选择是，我们边走边存，如下所示:

    ```py
    def add_sample_csv(gene_list):
        gene_count = load('my_genes.csv')
        for gene in gene_list:
            gene_count[gene]=gene_count(gene,0)+1
        save(gene_count, 'my_genes.csv')

    add_sample_csv(['MC4R', 'TYR'])
    add_sample_csv(['LCT', 'HLA-A'])
    add_sample_csv(['HLA-B', 'HLA-C'])
    ```

每当我们添加一个样本，我们就更新主数据库。虽然从 I/O 的角度来看，这种解决方案不是非常有效，但这不是这里要讨论的问题——实际上，从 I/O 的角度来看，大多数这种设计模式往往比我们接下来要看的更有效。

1.  当我们在最后存储最终数据时，第二个选择如下:

    ```py
    def add_sample_new_dict(dict_db, gene_list):
        my_dict_db = dict(dict_db)  # next recipe
        for gene in gene_list:
            dict_db.get(gene,0) +1
        return my_dict_db

    gene_count = load('my_genes.csv')
    gene_count = add_sample_new_dict(gene_count, ['MC4R', 'TYR'])
    gene_count = add_sample_new_dict(gene_count, ['LCT', 'HLA-A'])
    gene_count = add_sample_new_dict(gene_count, ['HLA-B', 'HLA-C'])
    save(gene_count, 'my_genes.csv')
    ```

在这种情况下，我们只在最后更新主数据库。我们使用内存字典来维护中间数据。

## 还有更多...

那么，为什么要这样做呢？延迟写入最终数据的解决方案更加复杂，因为我们必须潜在地存储部分结果并对其进行管理(在这种情况下，我们使用`gene_count`字典——这是一个简单的例子)。事实上，这是真的，但代码有 bug，磁盘存储空间填满，计算机在现实世界中会崩溃，所以从务实的角度来看，面对这一点是更重要的考虑因素。

假设您的第一个解决方案由于某种原因在执行过程中停止了工作。数据库处于未知状态，因为您不知道代码在哪里。您不能从零开始重新启动代码，因为它的一部分可能已经被执行了。因此，您必须检查数据库的状态，查看代码在哪里停止，并部分运行丢失的部分，或者找到一种方法回滚执行的部分。它既麻烦又容易出错。

现在，假设在第二种方法的执行过程中出现了一个问题:您不必担心数据的中间状态，因为它不会影响您的最终数据库，并且会在第二次执行中完全重新计算。您可以重新运行代码，而不用担心不一致的状态。唯一的例外是最后一行，但这不仅不太常见，而且您还可以将所有精力集中在这一点上，以使流程尽可能具有弹性。

这能一直做到吗？还是只在某些情况下有效？大多数代码都可以创建临时文件。此外，许多生物信息学分析不是实时事务性的，所以通常只在最后才更新 SQL 数据库很容易。现在，您有了一个与整个代码相关的单点故障——如果有问题，您知道您只需要关注最后的部分。

在可能的情况下，这种方法将使复杂代码的日常维护更加容易。

# 使用惰性编程进行流水线操作

懒惰编程是一种策略，我们把计算推迟到真正需要的时候。与急切编程相比，它有许多优点，在急切编程中，我们一调用计算就计算所有的东西。

Python 为懒惰编程提供了许多机制——事实上，从 Python 2 到 Python 3 的最大变化之一就是语言变得更加懒惰。

为了理解懒惰编程，我们将再次使用我们的基因数据库，并用它做一个练习。我们将检查是否至少有 *n* 个基因，每个基因有 *y* 个读数(例如，三个基因，每个基因有五个读数)。比方说，这可以是对我们数据库质量的一种衡量——也就是说，衡量我们是否有足够的基因和一定数量的样本。

我们将考虑两种实现:一种是懒惰的，另一种是渴望的。然后我们将比较这两种方法的含义。

## 准备就绪

该配方的代码可在`Chapter12/Lazy.py`中找到。

## 怎么做...

看看这两种不同的方法:

1.  让我们从急切的实现开始:

    ```py
    import pandas as pd
    def load(file_name):
        df = pd.read_csv(file_name).set_index('gene')
        return dict(df['count'])

    def get_min_reads(all_data, min_reads):
        return {
            gene: count
            for gene, count in all_data.items()
            if count >= min_reads
        }

    def has_min_observations(subset_data, min_observations):
        return len(subset_data) >= min_observations
    print(has_min_observations(
        get_min_reads(
            load('my_genes.csv'), 4
        ), 3))
    ```

注意代码加载所有数据，检查所有条目的计数`4`，并查看是否至少有`3`个具有该计数的观察值。

1.  这是一个另类，懒惰的版本:

    ```py
    def get_rec(file_name):
        with open(file_name) as f:
            f.readline()  # header
            for line in f:
                toks = line.strip().split(',')
                yield toks[0], int(toks[1])

    def gene_min_reads(source, min_reads):
        for gene, count in source:
            if count >= min_reads:
                yield gene

    def gene_min_observations(subset_source, min_observations):
        my_observations = 0
        for gene in subset_source:
            my_observations += 1
            if my_observations == min_observations:
                return True
        return False
    print(gene_min_observations(
        gene_min_reads(
            get_rec('my_genes.csv'), 4
        ), 2))
    ```

这段代码以一种完全不同的方式运行。首先，`get_rec`和`gene_min_reads`是生成器(注意`yield`，所以它们只会在需要的时候返回结果，一个接一个。`gene_min_observations`一旦观察到所需的观察次数，就会返回。这意味着将被读取和处理的唯一数据是达到结果的最少数据。在最坏的情况下，这仍然是整个文件，但在许多情况下，它可以少得多。

## 还有更多...

对于非常大的文件，这种方法的优势显而易见。虽然我们的玩具文件没有明显的优势，但是如果文件太大而不适合内存，第一种方法就会崩溃！此外，管道的每一步都需要足够的内存来存储它消耗的所有数据，并且所有的步骤都需要同时在内存中。因此，即使对于中等大小的文件，内存问题也可能在急切版本中出现，而惰性版本可以避免这种问题。

很多时候，惰性代码做的处理也少得多:就像前面的例子一样，它可能会在看到所有数据之前就停止计算。这并不总是确定的，但在许多用例中都会发生。

从历史的角度来看，Python 2 比 Python 3 更有热情。这实际上是从 Python 2 到 Python 3 的主要变化之一。举个简单的例子，考虑一下 Python 2 和 3 中`range`的行为。

我们的惰性实现的一部分可以通过使用一些 Python 内置模块以更简洁的方式编写——我们将在最终的食谱中再次讨论这一点。

# 使用 Python 递归的局限性

递归——一个调用自己的函数——是函数式编程中的一个基本概念。许多迎合函数式编程的语言能够无限递归。Python 不是这些语言中的一种。你可以在 Python 中使用递归，但是你必须意识到它的局限性，即递归会导致相当大的内存开销，并且它不能用来代替迭代——这是编写函数式程序时的典型情况。

为了了解 Python 递归的限制，我们将以几种方式实现斐波那契数列。提醒一下，斐波那契可以定义如下:

```py
Fib(0) = 0
Fib(1) = 1
Fib(n) = Fib(n -1) + Fib(n –2)
```

我们也在计算阶乘函数，但在这种情况下只是以递归的方式:

```py
Fact(1) = 1
Fact(n) = n * Fact(n –1 )
```

## 准备就绪

我们的代码可以在`Chapter12/Recursion.py`中找到。

## 怎么做...

按照以下步骤开始:

1.  先说一个迭代版的斐波那契:

    ```py
    def fibo_iter(n):
        if n < 2:
            return n
        last = 1
        second_last = 0
        for _i in range(2, n + 1):
            result = second_last + last
            second_last = last
            last = result
        return result
    ```

这个版本已经足够了，而且也非常高效。我们并不是说迭代版本在任何方面都不如递归版本。相反，用 Python，可能会表现得更好。

1.  下面是一个递归版本:

    ```py
    def fibo_naive(n):
        if n == 0:
            return 0
        if n == 1:
            return 1
        return fibo_naive(n - 1) + fibo_naive(n - 2)
    ```

如果您运行它，您会发现与迭代版本相比，性能受到了相当大的影响。此外，如果您要求`fibo_naive(1000)`或一个类似的大数字，代码将根本不能很好地执行(1000 个案例可能需要很多小时)，而迭代版本将充分执行。在接下来的食谱中，我们将实际修复其中的一部分。但是现在，让我们更深入地研究递归和 Python 的问题。

1.  为了让的情况变得非常明显，让我们用实现一个阶乘函数，从递归实现的角度来看，这个函数尽可能简单(甚至比斐波那契更简单):

    ```py
    def factorial(n):
        if n == 1:
            return 1
        return n * factorial(n - 1)
    ```

如果你用一个小数字运行这个函数，比如`factorial(5)`，你会得到正确的答案，`5`。但是如果你尝试一个大的数字，比如`factorial(20000)`，你会得到如下结果:

```py
Traceback (most recent call last):
  File "Recursion.py", line 8, in <module>
    factorial(20000)
  File "Recursion.py", line 4, in factorial
    return n * factorial(n - 1)
  File "Recursion.py", line 4, in factorial
    return n * factorial(n - 1)
  File "Recursion.py", line 4, in factorial
    return n * factorial(n - 1)
  [Previous line repeated 995 more times]
  File "Recursion.py", line 2, in factorial
    if n == 1:
RecursionError: maximum recursion depth exceeded in comparison
```

这个错误提示在 Python 中你可以递归多少是有限制的:Python 只能递归到某个限制(见`sys.setrecursionlimit`来改变这一点)。虽然您可以将递归的数量变得更大，但是维护堆栈所需的内存总会有一个限制。还有其他方法可以显式实现递归，但代价是速度。许多语言都实现了一个称为**尾部调用优化** ( **TCO** )的特性，它允许高性能的无限级递归，但是 Python 没有实现它。

## 还有更多...

在 Python 中，您可以对不需要多次循环调用的简单情况使用递归，但是在 Python 中，递归并不是迭代解决方案的一般替代品。递归可能是 Python 函数世界中实现得最差的主要特性。

# Python 的 functools 模块展示

Python 有三个内置模块，在用函数式方言编写代码时非常有用:`functools`、`operator`和`itertools`。在本食谱中，我们将简要讨论`functools`模块。例如，基本的`reduce`功能(这是 MapReduce 名字的一部分)只有在导入`functools`时才可用。

虽然对这些模块的详细探究对于单个配方来说太长了，但是我们将通过使用`functools`中的功能改进之前配方的一些代码来展示一些功能，并展示模块效用的一些说明性示例。

## 准备就绪

我们的代码可以在`Chapter12/Builtin.py`中找到。我们将参考以前的食谱。

## 怎么做...

让我们看几个说明性的例子:

1.  还记得我们在前面的配方中阶乘函数的递归实现不是很高效吗？让我们用一种非常简单的方式来缓解它的许多问题:

    ```py
    import functools

    @functools.cache
    def fibo(n):
        if n == 0:
            return 0
        if n == 1:
            return 1
        return fibo(n - 1) + fibo(n - 2)
    ```

如果你记得前一个配方中的递归代码，它非常慢，即使对于小数字也是如此。做这件事可能需要几个小时。这个函数只需添加`@functools.cache`装饰器，就可以更快地处理更多的数字。这是由于`cache`装饰器实现了记忆化。

什么是记忆化？

记忆化是这样一个过程，计算机通过它缓存执行的结果，并在下一次调用它时，不再次计算它，而只是返回存储的结果。例如，第一次调用`fibo(10)`，为了得到结果，实际上执行了函数，但是第二次调用时，结果在第一次运行时被缓存，没有执行就返回了。只有当函数总是为相等的输入返回相同的输出，并且没有副作用时，记忆化才能正确工作。也就是说，记忆化只对纯函数起作用。

1.  对于另一个使用函数方法替代现有代码的示例，从*使用惰性编程进行流水线操作*方法:

    ```py
    def gene_min_reads(source, min_reads):
        for gene, count in source:
            if count >= min_reads:
                yield gene
    ```

    中获取函数

这里已经有了一些功能性的味道，因为我们使用了一个生成器。

1.  这个函数可以用一种功能性更强的方言来编写:

    ```py
    def gene_min_reads(source, min_reads):
        return map(
            lambda x: x[0],
            filter(lambda x: x[1] >= min_reads,
            source.items()))
    ```

这里有很多东西需要打开。首先看内置的`filter`函数:它会将第一个参数中定义的函数应用到第二个参数上迭代器的对象上。在我们的例子中，第二个元素大于或等于`min_reads`的对象将被保留。然后，`map`函数获取每个对象(类型为`('GENE', count)`)并只返回第一部分。`map`和`filter`函数在函数式编程的方言中非常常见。还要注意匿名函数的典型函数概念，a `lambda`:这是一个只在一个地方使用的函数定义——它对非常小的函数非常有用。在这种情况下，没有直接的理由以这种方式重写(特别是因为前面定义的生成器类型已经为我们的问题提供了函数式方言的最重要的特性)，但是您会发现在许多情况下这种类型的表示更加简洁和有表现力。

1.  另一个使用分布式系统时你可能需要的重要概念是部分函数应用——这里有一个使用最基本算术函数的简单例子:

    ```py
    def multiplication(x, y):
        return x * y
    double = functools.partial(multiplication, 2)
    double(3)
    ```

`double`被定义为部分解决`multiplication`中的一个参数——因此，我们使用术语部分函数应用。这不仅仅是一个风格上的问题，因为有时在分布式平台中(或者甚至仅仅是多处理池的`map`函数)，您被限制为只能提供一个参数——因此，部分函数应用程序成为一种必要。

## 还有更多...

在`functools`模块中有许多更有趣的应用程序可以查看。从功能的角度来看，`itertools`和`operator`模块也有许多惯用的功能。

## 参见...

Python 有几个函数式编程库，例如，参见以下内容:

*   toolz(http://toolz.readthedocs.org/ ),带有额外的迭代器、函数实用程序和字典的函数式方言。
*   Pyrsistent 实现了不可变的数据结构:[`github.com/tobgu/pyrsistent.`](https://github.com/tobgu/pyrsistent%0D)
*   关于 Python 函数式编程的一般资源，请务必查看 GitHub 上令人敬畏的函数式 Python 列表:[`github.com/sfermigier/awesome-functional-python`](https://github.com/sfermigier/awesome-functional-python)。